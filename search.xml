<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用ImageNet预训练模型权重</title>
      <link href="/posts/af40.html"/>
      <url>/posts/af40.html</url>
      
        <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>使用ImageNet预训练模型权重<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#使用ImageNet预训练模型权重" target="_blank" rel="noopener"></a></p><p>Keras框架提供了预置好的VGG16模型，并提供使用ImageNet进行大规模训练的权重。我们使用预置的VGG16模型，加载预训练的权重，保留VGG16的卷积网络结构，只在最后的全连接层进行调整，使用GlobalAveragePooling2D将7x7x512的卷积结果进行全局池化，减少训练参数，并加入softmax激活、output shape为2的全连接层进行二分类。</p><p>首先，我们准备数据：</p><p>In [1]:</p><pre><code>!pip install --upgrade keras_applications==1.0.6 keras==2.2.4import osif os.path.exists(&#39;./data&#39;) == False:    from modelarts.session import Session    session = Session()    if session.region_name == &#39;cn-north-1&#39;:        bucket_path=&quot;modelarts-labs/notebook/DL_image_recognition/image_recognition.tar.gz&quot;    elif session.region_name == &#39;cn-north-4&#39;:        bucket_path=&quot;modelarts-labs-bj4/notebook/DL_image_recognition/image_recognition.tar.gz&quot;    else:        print(&quot;请更换地区到北京一或北京四&quot;)    session.download_data(    bucket_path=bucket_path,    path=&quot;./image_recognition.tar.gz&quot;)    # 使用tar命令解压资源包    !tar xf ./image_recognition.tar.gz    # 清理压缩包    !rm -f ./image_recognition.tar.gz```</code></pre><p>Looking in indexes: <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a><br>Requirement already up-to-date: keras_applications==1.0.6 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (1.0.6)<br>Requirement already up-to-date: keras==2.2.4 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (2.2.4)<br>Requirement already satisfied, skipping upgrade: h5py in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras_applications==1.0.6) (2.9.0)<br>Requirement already satisfied, skipping upgrade: numpy&gt;=1.9.1 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras_applications==1.0.6) (1.15.4)<br>Requirement already satisfied, skipping upgrade: six&gt;=1.9.0 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.12.0)<br>Requirement already satisfied, skipping upgrade: keras-preprocessing&gt;=1.0.5 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.0.5)<br>Requirement already satisfied, skipping upgrade: scipy&gt;=0.14 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.2.0)<br>Requirement already satisfied, skipping upgrade: pyyaml in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (3.13)</p><pre><code>### 导入相关库[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#导入相关库)In \[2\]:</code></pre><p>from keras.applications.vgg16 import VGG16from keras.preprocessing import imageimport numpy as np</p><p>from keras.preprocessing import imagefrom keras.models import Modelfrom keras.layers import Dense, GlobalAveragePooling2Dfrom keras import backend as Kfrom keras.models import load_model</p><p>from keras.preprocessing.image import ImageDataGenerator```</p><pre><code>Using TensorFlow backend.</code></pre><h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#准备数据集" target="_blank" rel="noopener"></a></h3><p>In [3]:</p><pre><code>import osfrom PIL import Imagedef load_data():    dirname = &quot;./data&quot;    path = &quot;./data&quot;    num_train_samples = 25000    x_train = np.empty((num_train_samples, 224,224,3), dtype=&#39;uint8&#39;)    y_train = np.empty((num_train_samples,1), dtype=&#39;uint8&#39;)    index = 0    for file in os.listdir(&quot;./data&quot;):        image = Image.open(os.path.join(dirname,file)).resize((224,224))        image = np.array(image)        x_train[index,:,:,:] = image        if &quot;cat&quot; in file:            y_train[index,0] =1        elif &quot;dog&quot; in file:            y_train[index,0] =0        index += 1    return (x_train, y_train)```In \[4\]:</code></pre><p>(x_train, y_train) = load_data()```<br>In [5]:</p><pre><code>print(x_train.shape)print(y_train.shape)```</code></pre><p>(25000, 224, 224, 3)<br>(25000, 1)</p><pre><code>In \[6\]:</code></pre><p>from keras.utils import np_utilsdef process_data(x_train,y_train):<br>    x_train = x_train.astype(np.float32)<br>    x_train /= 255<br>    n_classes = 2<br>    y_train = np_utils.to_categorical(y_train, n_classes)<br>    return x_train,y_train```<br>In [24]:</p><pre><code>def build_model(base_model):    x = base_model.output    x = GlobalAveragePooling2D()(x)    predictions = Dense(2, activation=&#39;softmax&#39;)(x)    model = Model(inputs=base_model.input, outputs=predictions)    print(type(model))    return model```In \[8\]:</code></pre><p>x_train,y_train= process_data(x_train,y_train)print(x_train.shape)print(y_train.shape)```</p><pre><code>(25000, 224, 224, 3)(25000, 2)</code></pre><h3 id="准备模型"><a href="#准备模型" class="headerlink" title="准备模型"></a>准备模型<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#准备模型" target="_blank" rel="noopener"></a></h3><p>我们定义输入数据的维度，并构建VGG16模型，加载ImageNet预训练的权重（如果预训练权重不存在，则从网络下载，并保存到到$HOME目录的.keras/models/路径下）。include_top=False表示只取卷积网络结构中的参数，不包含全连接层和softmax分类层（ImageNet有1000个分类）。</p><p>In [9]:</p><pre><code>base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False)```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Colocations handled automatically by placer.</p><pre><code>我们把所有卷基层的trainable设置为False，不进行训练。然后将池化层和全连接的二分类层添加到模型中，输入层不变In \[10\]:</code></pre><p>for layer in base_model.layers:<br>    layer.trainable = False</p><p>model = build_model(base_model)model.summary()```</p><pre><code>&lt;class &#39;keras.engine.training.Model&#39;&gt;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, None, None, 3)     0         _________________________________________________________________block1_conv1 (Conv2D)        (None, None, None, 64)    1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, None, None, 64)    36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, None, None, 64)    0         _________________________________________________________________block2_conv1 (Conv2D)        (None, None, None, 128)   73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, None, None, 128)   147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, None, None, 128)   0         _________________________________________________________________block3_conv1 (Conv2D)        (None, None, None, 256)   295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, None, None, 256)   0         _________________________________________________________________block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 2)                 1026      =================================================================Total params: 14,715,714Trainable params: 1,026Non-trainable params: 14,714,688_________________________________________________________________</code></pre><p>可以看到，训练的参数个数为1026，我们仅训练分类部分。</p><h3 id="设置模型的损失函数和优化器"><a href="#设置模型的损失函数和优化器" class="headerlink" title="设置模型的损失函数和优化器"></a>设置模型的损失函数和优化器<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#设置模型的损失函数和优化器" target="_blank" rel="noopener"></a></h3><p>In [11]:</p><pre><code>import keras opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)model.compile(loss=&#39;binary_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```### 设置callback并训练模型[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#设置callback并训练模型)使用预训练的权重，可以较快提高模型精度。在高性能GPU环境下，每一轮需要几分钟左右，请实践者保持耐心，笔者的实践中训练到第11轮就达到了约90%的精度，耗时约8分钟。后续训练精度提升开始变慢，在41轮达到约92%的精度，耗时约30分钟。In \[12\]:</code></pre><p>from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateaues = EarlyStopping(monitor=’val_acc’, baseline=0.9, patience=15, verbose=1, mode=’auto’)cp = ModelCheckpoint(filepath=”./ckpt_vgg16_dog_and_cat.h5”, monitor=”val_acc”, verbose=1, save_best_only=True, mode=”auto”, period=1)lr = ReduceLROnPlateau(monitor=”val_acc”, factor=0.1, patience=10, verbose=1, mode=”auto”, min_lr=0)callbacks = [es,cp,lr]```<br>In [13]:</p><pre><code>history = model.fit(x=x_train,                     y=y_train,                     batch_size=16,                     epochs=100,                     verbose=1,                     callbacks=callbacks,                     validation_split=0.25,                     shuffle=True,                     initial_epoch=0)```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Use tf.cast instead.<br>Train on 18750 samples, validate on 6250 samples<br>Epoch 1/100<br>18750/18750 [==============================] - 48s 3ms/step - loss: 0.6284 - acc: 0.6794 - val_loss: 0.5503 - val_acc: 0.7995</p><p>Epoch 00001: val_acc improved from -inf to 0.79952, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 2/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.4959 - acc: 0.8314 - val_loss: 0.4565 - val_acc: 0.8410</p><p>Epoch 00002: val_acc improved from 0.79952 to 0.84096, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 3/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.4233 - acc: 0.8564 - val_loss: 0.4012 - val_acc: 0.8630</p><p>Epoch 00003: val_acc improved from 0.84096 to 0.86304, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 4/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3789 - acc: 0.8707 - val_loss: 0.3669 - val_acc: 0.8710</p><p>Epoch 00004: val_acc improved from 0.86304 to 0.87104, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 5/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3493 - acc: 0.8793 - val_loss: 0.3417 - val_acc: 0.8824</p><p>Epoch 00005: val_acc improved from 0.87104 to 0.88240, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 6/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3274 - acc: 0.8848 - val_loss: 0.3233 - val_acc: 0.8867</p><p>Epoch 00006: val_acc improved from 0.88240 to 0.88672, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 7/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3105 - acc: 0.8903 - val_loss: 0.3089 - val_acc: 0.8864</p><p>Epoch 00007: val_acc did not improve from 0.88672<br>Epoch 8/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2975 - acc: 0.8933 - val_loss: 0.2966 - val_acc: 0.8909</p><p>Epoch 00008: val_acc improved from 0.88672 to 0.89088, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 9/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2867 - acc: 0.8960 - val_loss: 0.2869 - val_acc: 0.8979</p><p>Epoch 00009: val_acc improved from 0.89088 to 0.89792, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 10/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2778 - acc: 0.8975 - val_loss: 0.2797 - val_acc: 0.8990</p><p>Epoch 00010: val_acc improved from 0.89792 to 0.89904, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 11/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2702 - acc: 0.9004 - val_loss: 0.2721 - val_acc: 0.9016</p><p>Epoch 00011: val_acc improved from 0.89904 to 0.90160, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 12/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2637 - acc: 0.9020 - val_loss: 0.2658 - val_acc: 0.9011</p><p>Epoch 00012: val_acc did not improve from 0.90160<br>Epoch 13/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2581 - acc: 0.9029 - val_loss: 0.2605 - val_acc: 0.9034</p><p>Epoch 00013: val_acc improved from 0.90160 to 0.90336, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 14/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2530 - acc: 0.9045 - val_loss: 0.2559 - val_acc: 0.9045</p><p>Epoch 00014: val_acc improved from 0.90336 to 0.90448, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 15/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2486 - acc: 0.9055 - val_loss: 0.2518 - val_acc: 0.9064</p><p>Epoch 00015: val_acc improved from 0.90448 to 0.90640, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 16/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2445 - acc: 0.9074 - val_loss: 0.2480 - val_acc: 0.9070</p><p>Epoch 00016: val_acc improved from 0.90640 to 0.90704, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 17/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2410 - acc: 0.9084 - val_loss: 0.2447 - val_acc: 0.9083</p><p>Epoch 00017: val_acc improved from 0.90704 to 0.90832, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 18/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2377 - acc: 0.9090 - val_loss: 0.2417 - val_acc: 0.9083</p><p>Epoch 00018: val_acc did not improve from 0.90832<br>Epoch 19/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2346 - acc: 0.9106 - val_loss: 0.2398 - val_acc: 0.9078</p><p>Epoch 00019: val_acc did not improve from 0.90832<br>Epoch 20/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2319 - acc: 0.9115 - val_loss: 0.2362 - val_acc: 0.9107</p><p>Epoch 00020: val_acc improved from 0.90832 to 0.91072, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 21/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2293 - acc: 0.9118 - val_loss: 0.2339 - val_acc: 0.9107</p><p>Epoch 00021: val_acc did not improve from 0.91072<br>Epoch 22/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2270 - acc: 0.9122 - val_loss: 0.2318 - val_acc: 0.9114</p><p>Epoch 00022: val_acc improved from 0.91072 to 0.91136, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 23/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2247 - acc: 0.9132 - val_loss: 0.2299 - val_acc: 0.9114</p><p>Epoch 00023: val_acc did not improve from 0.91136<br>Epoch 24/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2228 - acc: 0.9135 - val_loss: 0.2278 - val_acc: 0.9125</p><p>Epoch 00024: val_acc improved from 0.91136 to 0.91248, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 25/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2209 - acc: 0.9147 - val_loss: 0.2265 - val_acc: 0.9122</p><p>Epoch 00025: val_acc did not improve from 0.91248<br>Epoch 26/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2190 - acc: 0.9151 - val_loss: 0.2243 - val_acc: 0.9120</p><p>Epoch 00026: val_acc did not improve from 0.91248<br>Epoch 27/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2173 - acc: 0.9155 - val_loss: 0.2227 - val_acc: 0.9141</p><p>Epoch 00027: val_acc improved from 0.91248 to 0.91408, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 28/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2158 - acc: 0.9163 - val_loss: 0.2213 - val_acc: 0.9142</p><p>Epoch 00028: val_acc improved from 0.91408 to 0.91424, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 29/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2142 - acc: 0.9166 - val_loss: 0.2199 - val_acc: 0.9139</p><p>Epoch 00029: val_acc did not improve from 0.91424<br>Epoch 30/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2129 - acc: 0.9174 - val_loss: 0.2187 - val_acc: 0.9144</p><p>Epoch 00030: val_acc improved from 0.91424 to 0.91440, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 31/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2114 - acc: 0.9179 - val_loss: 0.2173 - val_acc: 0.9149</p><p>Epoch 00031: val_acc improved from 0.91440 to 0.91488, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 32/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2101 - acc: 0.9169 - val_loss: 0.2170 - val_acc: 0.9154</p><p>Epoch 00032: val_acc improved from 0.91488 to 0.91536, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 33/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2090 - acc: 0.9179 - val_loss: 0.2151 - val_acc: 0.9170</p><p>Epoch 00033: val_acc improved from 0.91536 to 0.91696, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 34/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2079 - acc: 0.9193 - val_loss: 0.2145 - val_acc: 0.9173</p><p>Epoch 00034: val_acc improved from 0.91696 to 0.91728, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 35/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2068 - acc: 0.9182 - val_loss: 0.2130 - val_acc: 0.9171</p><p>Epoch 00035: val_acc did not improve from 0.91728<br>Epoch 36/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2057 - acc: 0.9198 - val_loss: 0.2122 - val_acc: 0.9176</p><p>Epoch 00036: val_acc improved from 0.91728 to 0.91760, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 37/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2047 - acc: 0.9196 - val_loss: 0.2114 - val_acc: 0.9187</p><p>Epoch 00037: val_acc improved from 0.91760 to 0.91872, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 38/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2038 - acc: 0.9199 - val_loss: 0.2103 - val_acc: 0.9182</p><p>Epoch 00038: val_acc did not improve from 0.91872<br>Epoch 39/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2027 - acc: 0.9206 - val_loss: 0.2096 - val_acc: 0.9189</p><p>Epoch 00039: val_acc improved from 0.91872 to 0.91888, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 40/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2019 - acc: 0.9204 - val_loss: 0.2090 - val_acc: 0.9178</p><p>Epoch 00040: val_acc did not improve from 0.91888<br>Epoch 41/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2011 - acc: 0.9205 - val_loss: 0.2082 - val_acc: 0.9202</p><p>Epoch 00041: val_acc improved from 0.91888 to 0.92016, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 42/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2003 - acc: 0.9218 - val_loss: 0.2072 - val_acc: 0.9195</p><p>Epoch 00042: val_acc did not improve from 0.92016<br>Epoch 43/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1994 - acc: 0.9220 - val_loss: 0.2068 - val_acc: 0.9202</p><p>Epoch 00043: val_acc did not improve from 0.92016<br>Epoch 44/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1987 - acc: 0.9216 - val_loss: 0.2059 - val_acc: 0.9184</p><p>Epoch 00044: val_acc did not improve from 0.92016<br>Epoch 45/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1979 - acc: 0.9214 - val_loss: 0.2065 - val_acc: 0.9189</p><p>Epoch 00045: val_acc did not improve from 0.92016<br>Epoch 46/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1960 - acc: 0.9229 - val_loss: 0.2033 - val_acc: 0.9194</p><p>Epoch 00048: val_acc did not improve from 0.92048<br>Epoch 49/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1953 - acc: 0.9226 - val_loss: 0.2037 - val_acc: 0.9192</p><p>Epoch 00049: val_acc did not improve from 0.92048<br>Epoch 50/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1946 - acc: 0.9226 - val_loss: 0.2029 - val_acc: 0.9189</p><p>Epoch 00050: val_acc did not improve from 0.92048<br>Epoch 51/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1942 - acc: 0.9232 - val_loss: 0.2021 - val_acc: 0.9211</p><p>Epoch 00051: val_acc improved from 0.92048 to 0.92112, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 52/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1934 - acc: 0.9231 - val_loss: 0.2015 - val_acc: 0.9211</p><p>Epoch 00052: val_acc did not improve from 0.92112<br>Epoch 53/100<br>13264/18750 [====================&gt;………] - ETA: 9s - loss: 0.1921 - acc: 0.9226```</p><pre><code>IOPub message rate exceeded.The notebook server will temporarily stop sending outputto the client in order to avoid crashing it.To change this limit, set the config variable`--NotebookApp.iopub_msg_rate_limit`.Current values:NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)NotebookApp.rate_limit_window=3.0 (secs)</code></pre><pre><code>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1882 - acc: 0.9254 - val_loss: 0.1966 - val_acc: 0.9210Epoch 00063: val_acc did not improve from 0.92208Epoch 64/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1876 - acc: 0.9256 - val_loss: 0.1963 - val_acc: 0.9210Epoch 00064: val_acc did not improve from 0.92208Epoch 65/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1873 - acc: 0.9251 - val_loss: 0.1968 - val_acc: 0.9230Epoch 00065: val_acc improved from 0.92208 to 0.92304, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 66/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1870 - acc: 0.9254 - val_loss: 0.1958 - val_acc: 0.9205Epoch 00066: val_acc did not improve from 0.92304Epoch 67/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1866 - acc: 0.9256 - val_loss: 0.1953 - val_acc: 0.9214Epoch 00067: val_acc did not improve from 0.92304Epoch 68/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1862 - acc: 0.9265 - val_loss: 0.1956 - val_acc: 0.9230Epoch 00068: val_acc did not improve from 0.92304Epoch 69/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1858 - acc: 0.9265 - val_loss: 0.1947 - val_acc: 0.9213Epoch 00069: val_acc did not improve from 0.92304Epoch 70/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1854 - acc: 0.9255 - val_loss: 0.1950 - val_acc: 0.9229Epoch 00070: val_acc did not improve from 0.92304Epoch 71/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1852 - acc: 0.9268 - val_loss: 0.1941 - val_acc: 0.9216Epoch 00071: val_acc did not improve from 0.92304Epoch 72/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1848 - acc: 0.9268 - val_loss: 0.1940 - val_acc: 0.9232Epoch 00072: val_acc improved from 0.92304 to 0.92320, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 73/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1845 - acc: 0.9274 - val_loss: 0.1937 - val_acc: 0.9230Epoch 00073: val_acc did not improve from 0.92320Epoch 74/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1841 - acc: 0.9266 - val_loss: 0.1933 - val_acc: 0.9206Epoch 00074: val_acc did not improve from 0.92320Epoch 75/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1838 - acc: 0.9268 - val_loss: 0.1930 - val_acc: 0.9218Epoch 00075: val_acc did not improve from 0.92320Epoch 76/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1834 - acc: 0.9266 - val_loss: 0.1928 - val_acc: 0.9227Epoch 00076: val_acc did not improve from 0.92320Epoch 77/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1832 - acc: 0.9273 - val_loss: 0.1926 - val_acc: 0.9224Epoch 00077: val_acc did not improve from 0.92320Epoch 78/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1826 - acc: 0.9273 - val_loss: 0.1924 - val_acc: 0.9237Epoch 00078: val_acc improved from 0.92320 to 0.92368, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 79/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1824 - acc: 0.9277 - val_loss: 0.1921 - val_acc: 0.9229Epoch 00079: val_acc did not improve from 0.92368Epoch 80/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1821 - acc: 0.9285 - val_loss: 0.1920 - val_acc: 0.9234Epoch 00080: val_acc did not improve from 0.92368Epoch 81/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1820 - acc: 0.9279 - val_loss: 0.1916 - val_acc: 0.9232Epoch 00081: val_acc did not improve from 0.92368Epoch 82/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1816 - acc: 0.9272 - val_loss: 0.1924 - val_acc: 0.9238Epoch 00082: val_acc improved from 0.92368 to 0.92384, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 83/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1814 - acc: 0.9282 - val_loss: 0.1911 - val_acc: 0.9226Epoch 00083: val_acc did not improve from 0.92384Epoch 84/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1811 - acc: 0.9273 - val_loss: 0.1909 - val_acc: 0.9222Epoch 00084: val_acc did not improve from 0.92384Epoch 85/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1808 - acc: 0.9278 - val_loss: 0.1908 - val_acc: 0.9219Epoch 00085: val_acc did not improve from 0.92384Epoch 86/10018750/18750 [==============================] - 45s 2ms/step - loss: 0.1804 - acc: 0.9276 - val_loss: 0.1903 - val_acc: 0.9227Epoch 00087: val_acc did not improve from 0.92384Epoch 88/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1801 - acc: 0.9286 - val_loss: 0.1901 - val_acc: 0.9227Epoch 00088: val_acc did not improve from 0.92384Epoch 89/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1797 - acc: 0.9284 - val_loss: 0.1913 - val_acc: 0.9234Epoch 00089: val_acc did not improve from 0.92384Epoch 90/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1795 - acc: 0.9279 - val_loss: 0.1897 - val_acc: 0.9230Epoch 00090: val_acc did not improve from 0.92384Epoch 91/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1795 - acc: 0.9284 - val_loss: 0.1896 - val_acc: 0.9224Epoch 00091: val_acc did not improve from 0.92384Epoch 92/10010608/18750 [===============&gt;..............] - ETA: 14s - loss: 0.1796 - acc: 0.9285```</code></pre><p>IOPub message rate exceeded.<br>The notebook server will temporarily stop sending output<br>to the client in order to avoid crashing it.<br>To change this limit, set the config variable<br><code>--NotebookApp.iopub_msg_rate_limit</code>.</p><p>Current values:<br>NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)<br>NotebookApp.rate_limit_window=3.0 (secs)</p><pre><code></code></pre><p>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1779 - acc: 0.9288 - val_loss: 0.1884 - val_acc: 0.9232</p><p>Epoch 00098: val_acc did not improve from 0.92464<br>Epoch 99/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1776 - acc: 0.9291 - val_loss: 0.1882 - val_acc: 0.9237</p><p>Epoch 00099: val_acc did not improve from 0.92464<br>Epoch 100/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1774 - acc: 0.9292 - val_loss: 0.1880 - val_acc: 0.9243</p><p>Epoch 00100: val_acc did not improve from 0.92464</p><pre><code>In \[21\]:</code></pre><p>import matplotlib.pyplot as pltprint(history.history[‘val_acc’][0])# 绘制训练 &amp; 验证的准确率值plt.plot(history.history[‘acc’])plt.plot(history.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</p><pre><code>0.7995200000190735</code></pre><p><img src="" alt=""><br>In [22]:</p><pre><code># 绘制训练 &amp; 验证的损失值plt.plot(history.history[&#39;loss&#39;])plt.plot(history.history[&#39;val_loss&#39;])plt.title(&#39;Model loss&#39;)plt.ylabel(&#39;Loss&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()在上面的训练中，我们使用预训练模型的权重，并保留了卷积网络结构中的参数权重，只训练分类层的参数。可以看到，由于预训练模型已经具备了比较好的特征提取能力，初始时就能获得比较高的准确率。接下来，我们将卷积网络中的参数也一起加入训练。在预训练模型的特征提取能力的基础上开始训练，找到更适合猫狗二分类任务的参数权重：In \[31\]:</code></pre><p>base_model = VGG16(weights=’imagenet’, include_top=False)model = build_model(base_model)model.compile(loss=’binary_crossentropy’,<br>              optimizer=opt,<br>              metrics=[‘accuracy’])model.summary()```</p><pre><code>&lt;class &#39;keras.engine.training.Model&#39;&gt;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_5 (InputLayer)         (None, None, None, 3)     0         _________________________________________________________________block1_conv1 (Conv2D)        (None, None, None, 64)    1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, None, None, 64)    36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, None, None, 64)    0         _________________________________________________________________block2_conv1 (Conv2D)        (None, None, None, 128)   73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, None, None, 128)   147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, None, None, 128)   0         _________________________________________________________________block3_conv1 (Conv2D)        (None, None, None, 256)   295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, None, None, 256)   0         _________________________________________________________________block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________global_average_pooling2d_5 ( (None, 512)               0         _________________________________________________________________dense_5 (Dense)              (None, 2)                 1026      =================================================================Total params: 14,715,714Trainable params: 14,715,714Non-trainable params: 0_________________________________________________________________</code></pre><p>可以看到，这次模型的Non-trainable params个数为0，所有参数都参与训练，相当于我们使用预训练的参数权重初始化了卷积部分。</p><h3 id="训练所有参数"><a href="#训练所有参数" class="headerlink" title="训练所有参数"></a>训练所有参数<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#训练所有参数" target="_blank" rel="noopener"></a></h3><p>直接在预训练权重的基础上进行训练，可以更快的让泛化性较好的权重收敛到适合猫狗二分类的权重，我们仅训练3轮就达到了约95%的精度，耗时约5分钟。</p><p>In [32]:</p><pre><code>history = model.fit(x=x_train,                     y=y_train,                     batch_size=16,                     epochs=5,                     verbose=1,                     callbacks=callbacks,                     validation_split=0.25,                     shuffle=True,                     initial_epoch=0)```</code></pre><p>Train on 18750 samples, validate on 6250 samples<br>Epoch 1/5<br>18750/18750 [==============================] - 103s 5ms/step - loss: 0.4086 - acc: 0.8029 - val_loss: 0.2317 - val_acc: 0.8989</p><p>Epoch 00001: val_acc did not improve from 0.95120<br>Epoch 2/5<br>18750/18750 [==============================] - 102s 5ms/step - loss: 0.2157 - acc: 0.9247 - val_loss: 0.2462 - val_acc: 0.9019</p><p>Epoch 00002: val_acc did not improve from 0.95120<br>Epoch 3/5<br>18750/18750 [==============================] - 101s 5ms/step - loss: 0.1959 - acc: 0.9379 - val_loss: 0.1228 - val_acc: 0.9555</p><p>Epoch 00003: val_acc improved from 0.95120 to 0.95552, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 4/5<br>18750/18750 [==============================] - 101s 5ms/step - loss: 0.1849 - acc: 0.9386 - val_loss: 0.3406 - val_acc: 0.9373</p><p>Epoch 00004: val_acc did not improve from 0.95552<br>Epoch 5/5<br>18750/18750 [==============================] - 101s 5ms/step - loss: 0.2295 - acc: 0.9324 - val_loss: 0.1599 - val_acc: 0.9496</p><p>Epoch 00005: val_acc did not improve from 0.95552</p><pre><code>In \[33\]:</code></pre><p>import matplotlib.pyplot as pltprint(history.history[‘val_acc’][0])# 绘制训练 &amp; 验证的准确率值plt.plot(history.history[‘acc’])plt.plot(history.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show();```</p><pre><code>0.8988799999618531</code></pre><p><img src="" alt=""><br>可以看到，模型在开始训练后就得到了较高的精度。在VGG论文中，作者也提到了使用预训练好的浅层VGG模型去初始化深层VGG模型的一部分层，再继续训练的技巧。在上面的实践中，我们使用了泛化性更强的参数权重初始化模型，并针对我们特定的任务进行训练，可以更快的得到较好的精度。</p><h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#扩展" target="_blank" rel="noopener"></a></h3><p>尝试使用更深层的网络，如VGG19, ResNet在ImageNet数据集预训练的权重，并针对猫狗二分类任务进行迁移学习，查看模型训练的效果。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> modelarts </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据增广</title>
      <link href="/posts/abe6.html"/>
      <url>/posts/abe6.html</url>
      
        <content type="html"><![CDATA[<h1 id="数据增广-Data-Augumentation"><a href="#数据增广-Data-Augumentation" class="headerlink" title="数据增广(Data Augumentation)"></a>数据增广(Data Augumentation)<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30325f646174615f617567756d656e746174696f6e2e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/02_data_augumentation.ipynb&repository_id=185701977&repository_type=Repository#数据增广%28Data-Augumentation%29" target="_blank" rel="noopener"></a></h1><p>在VGG论文中，作者提到了数据增广(Data Augumentation)的重要性。在原论文中，作者主要对输入数据的尺寸进行了调整，使用了原始尺寸、较大尺寸和较小尺寸三种规格，并获得了一定的准确率的提升(具体请参考<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">论文</a>或<a href="https://github.com/huaweicloud/ModelArts-Lab/wiki/VGG模型解析" target="_blank" rel="noopener">VGG模型解析文章</a>)。在Keras中，我们通过DataGenerator，对输入的图片进行更丰富的预处理，包括缩放、旋转、平移、水平翻转等，来探索数据增广对模型精度带来的提升。预处理并不会直接增加数据集的样本数量，而是让样本具备更加丰富的特征，理论上，可以让训练的模型获得更好的泛化性。</p><p>首先，我们准备数据：</p><p>In [1]:</p><pre><code>!pip install --upgrade keras_applications==1.0.6 keras==2.2.4import osif os.path.exists(&#39;./data&#39;) == False:    from modelarts.session import Session    session = Session()    if session.region_name == &#39;cn-north-1&#39;:        bucket_path=&quot;modelarts-labs/notebook/DL_image_recognition/image_recognition.tar.gz&quot;    elif session.region_name == &#39;cn-north-4&#39;:        bucket_path=&quot;modelarts-labs-bj4/notebook/DL_image_recognition/image_recognition.tar.gz&quot;    else:        print(&quot;请更换地区到北京一或北京四&quot;)    session.download_data(    bucket_path=bucket_path,    path=&quot;./image_recognition.tar.gz&quot;)    # 使用tar命令解压资源包    !tar xf ./image_recognition.tar.gz    # 清理压缩包    !rm -f ./image_recognition.tar.gz```</code></pre><p>Looking in indexes: <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a><br>Requirement already up-to-date: keras_applications==1.0.6 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (1.0.6)<br>Requirement already up-to-date: keras==2.2.4 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (2.2.4)<br>Requirement already satisfied, skipping upgrade: numpy&gt;=1.9.1 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras_applications==1.0.6) (1.15.4)<br>Requirement already satisfied, skipping upgrade: h5py in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras_applications==1.0.6) (2.9.0)<br>Requirement already satisfied, skipping upgrade: six&gt;=1.9.0 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.12.0)<br>Requirement already satisfied, skipping upgrade: pyyaml in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (3.13)<br>Requirement already satisfied, skipping upgrade: keras-preprocessing&gt;=1.0.5 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.0.5)<br>Requirement already satisfied, skipping upgrade: scipy&gt;=0.14 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.2.0)<br>Successfully download file modelarts-labs/notebook/DL_image_recognition/image_recognition.tar.gz from OBS to local ./image_recognition.tar.gz</p><pre><code>原有的数据存放在data/目录下，按照文件名区分类别。现在，我们按照下面的目录结构重新组织数据：</code></pre><p>data-for-gen/<br>├── train<br>│   ├── cat<br>│   │   ├── cat.1.jpg<br>│   │   └── cat.1589.jpg<br>│   └── dog<br>│       ├── dog.265.jpg<br>│       └── dog.9841.jpg<br>└── val<br>    ├── cat<br>    │   ├── cat.10006.jpg<br>    │   └── cat.999.jpg<br>    └── dog<br>        ├── dog.10003.jpg<br>        └── dog.9999.jpg</p><pre><code>我们利用Linux系统中的软连接机制，直接创建文件目录，并通过软连接连接到原有的数据中，减少磁盘的占用。In \[2\]:</code></pre><p>from keras.preprocessing.image import ImageDataGeneratorfrom glob import glob</p><p>ROWS = 224COLS = 224</p><p>if os.path.exists(‘./data-for-gen’) == False:<br>    !mkdir -p data-for-gen/train/dog<br>    !mkdir -p data-for-gen/train/cat<br>    !mkdir -p data-for-gen/val/dog<br>    !mkdir -p data-for-gen/val/cat</p><pre><code>DATA_DIR = &#39;./data-for-gen/&#39; # 数据集路径dog_glob = glob(&#39;./data/dog*.jpg&#39;)cat_glob = glob(&#39;./data/cat*.jpg&#39;)# 我们使用25%的数据作为验证集：val_split = 0.25index = int(len(dog_glob) * val_split)def gen_lnk_cmds(class_name, class_glob):    cmds = &#39;&#39;    for i in range(len(class_glob)):        filename = os.path.basename(class_glob[i])        src_path = os.path.realpath(class_glob[i])        sample_type = &#39;train&#39; if i &gt; index else &#39;val&#39;        lnk_path = os.path.realpath(&#39;./data-for-gen/{}/{}/{}&#39;.format(sample_type, class_name, filename))        cmds = cmds + &#39;ln -s {} {}\n&#39;.format(src_path, lnk_path)    return cmds# 准备dog图片print(&#39;prepare dog images for data augumentation&#39;)with open(&#39;./tmp_gen_dogs.sh&#39;, &#39;w&#39;) as f:    link_cmds = gen_lnk_cmds(&#39;dog&#39;, dog_glob)    f.write(link_cmds)    !sh ./tmp_gen_dogs.sh    !rm ./tmp_gen_dogs.sh# 准备cat图片print(&#39;prepare cat images for data augumentation&#39;)with open(&#39;./tmp_gen_cats.sh&#39;, &#39;w&#39;) as f:    link_cmds = gen_lnk_cmds(&#39;cat&#39;, cat_glob)    f.write(link_cmds)    !sh ./tmp_gen_cats.sh    !rm ./tmp_gen_cats.sh```</code></pre><pre><code>Using TensorFlow backend.</code></pre><p>下面，我们为训练集和验证集创建ImageDataGenerator。ImageDataGenerator中包含了丰富的参数，这里我们仅使用了其中的几项，更多详细的参数，可以参考Keras的文档<a href="https://keras.io/preprocessing/image/#imagedatagenerator-class" target="_blank" rel="noopener">ImageDataGenerator</a>。</p><p>In [3]:</p><pre><code>from PIL import Imageimport matplotlib.pyplot as pltimport numpy as np%matplotlib inlinetrain_datagen = temp_val_datagen = ImageDataGenerator(    rescale=1.0/255,  # ImageDataGenerator使用[0-1]表示RGB色值，加入rescale以正常显示图片    rotation_range=10, # 旋转角度范围，以角度为单位    width_shift_range=0.2, # 宽度的偏移范围    height_shift_range=0.2, # 高度的偏移范围    shear_range=0.1, # 图片裁剪范围    zoom_range=0.1, # 图片缩放范围    fill_mode=&#39;nearest&#39;, # 图片处理后的填充模式    horizontal_flip=True) # 是否随机水平翻转图片val_datagen = temp_val_datagen = ImageDataGenerator(    rescale=1.0/255,    rotation_range=10,    width_shift_range=0.2,    height_shift_range=0.2,    shear_range=0.1,    zoom_range=0.1,    fill_mode=&#39;nearest&#39;,    horizontal_flip=True)```在训练模型之前，我们首先查看ImageDataGenerator的效果，我们使用一张训练集的dog图片作为原图，查看图片增广的效果：In \[4\]:</code></pre><p>temp_val_datagen = ImageDataGenerator(<br>    rescale=1.0/255,<br>    rotation_range=10,<br>    width_shift_range=0.2,<br>    height_shift_range=0.2,<br>    shear_range=0.1,<br>    zoom_range=0.1,<br>    fill_mode=’nearest’,<br>    horizontal_flip=True)</p><p>from PIL import Imagesample_img = Image.open(glob(‘./data/dog*.jpg’)[0])sample_img = np.expand_dims(sample_img, axis=0)</p><p>i = 0plt.figure(figsize=(20, 10))for batch in temp_val_datagen.flow(sample_img):<br>    plt.subplot(2, 4, i+1)<br>    plt.axis(‘off’)<br>    plt.imshow(np.squeeze(batch))</p><pre><code>i += 1if i &gt;= 8:    break</code></pre><p>plt.show()```<br><img src="" alt=""><br>下面，我们调用ImageDataGenerator的flow_from_directory来构造训练集和验证集的generator。注意此处需要指定target_size，这里的方法与VGG论文中有所不同，我们虽然固定了尺寸，但是通过缩放等操作对局部特征进行了缩放，与直接进行原图的缩放效果是相似的。</p><p>In [5]:</p><pre><code>train_generator = train_datagen.flow_from_directory(&#39;./data-for-gen/train&#39;,                                                     target_size=(ROWS, COLS), batch_size=16, class_mode=&#39;binary&#39;)val_generator = val_datagen.flow_from_directory(&#39;./data-for-gen/val&#39;,                                                 target_size=(ROWS, COLS), batch_size=16, class_mode=&#39;binary&#39;)```</code></pre><p>Found 18748 images belonging to 2 classes.<br>Found 6252 images belonging to 2 classes.</p><pre><code>下面，我们使用keras.applications包中的VGG16模型，进行训练。### 构建VGG16模型[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30325f646174615f617567756d656e746174696f6e2e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/02_data_augumentation.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#构建VGG16模型)In \[6\]:</code></pre><p>from keras.applications.vgg16 import VGG16from keras.models import Modelfrom keras.layers import Flatten, Dense, GlobalAveragePooling2Dfrom keras.optimizers import RMSprop</p><p>optimizer = RMSprop(lr=1e-4, decay=1e-6) # 优化器使用RMSprop, 设置学习率是1e-4objective = ‘binary_crossentropy’ # loss 函数使用交叉熵</p><p>base_model = VGG16(weights=None, include_top=False, input_shape=(COLS, ROWS, 3))</p><p>x = base_model.outputx = GlobalAveragePooling2D()(x)</p><p>output = Dense(1, activation=’sigmoid’)(x)</p><p>model = Model(input=base_model.input, output=output)model.summary()</p><p>model.compile(loss=objective, optimizer=optimizer, metrics=[‘accuracy’])```</p><pre><code>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.Instructions for updating:Colocations handled automatically by placer._________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 224, 224, 3)       0         _________________________________________________________________block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         _________________________________________________________________block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         _________________________________________________________________block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         _________________________________________________________________block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         _________________________________________________________________block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 1)                 513       =================================================================Total params: 14,715,201Trainable params: 14,715,201Non-trainable params: 0_________________________________________________________________</code></pre><pre><code>/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(&quot;in..., outputs=Tensor(&quot;de...)`  app.launch_new_instance()</code></pre><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30325f646174615f617567756d656e746174696f6e2e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/02_data_augumentation.ipynb&repository_id=185701977&repository_type=Repository#训练模型" target="_blank" rel="noopener"></a></h3><p>由于数据增广需要对图像进行旋转、缩放等预处理，因此每一轮的训练时间会更久，<strong>在高性能GPU环境下，每一轮需要几分钟左右，请实践者保持耐心，笔者的实践中，经过44轮的训练精度达到约94%，耗时约3.5小时</strong>。在实践时，我们可以尝试减少ImageDataGenerator的预处理参数，专注于某一项或几项增广对训练产生的影响。也可以适当的减少EarlyStopping的patience，让训练更早的结束，以减少训练时间。</p><p>In [ ]:</p><pre><code>epochs = 50 # 训练轮数from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau# early stopping策略early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=20, verbose=1, mode=&#39;auto&#39;)   mcp = ModelCheckpoint(&#39;aug.weights.{epoch:03d}_{acc:.4f}_{val_acc:.4f}.h5&#39;,                       monitor=&#39;val_acc&#39;, save_best_only=True, save_weights_only=True, verbose=1)reduce_lr = ReduceLROnPlateau(monitor=&#39;val_acc&#39;, factor=0.2, patience=5, min_lr=1e-9, verbose=1)# 开始训练hist = model.fit_generator(    epochs=epochs,    generator=train_generator,    steps_per_epoch=len(train_generator),    validation_data=val_generator,    validation_steps=len(val_generator),    shuffle=True,    callbacks=[early_stopping, mcp, reduce_lr])```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Use tf.cast instead.<br>Epoch 1/1000<br>1172/1172 [==============================] - 290s 248ms/step - loss: 0.6736 - acc: 0.5851 - val_loss: 0.6458 - val_acc: 0.6470</p><p>Epoch 00001: val_acc improved from -inf to 0.64699, saving model to aug.weights.001_0.5851_0.6470.h5<br>Epoch 2/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.6298 - acc: 0.6499 - val_loss: 0.5784 - val_acc: 0.7033</p><p>Epoch 00002: val_acc improved from 0.64699 to 0.70329, saving model to aug.weights.002_0.6498_0.7033.h5<br>Epoch 3/1000<br>1172/1172 [==============================] - 283s 241ms/step - loss: 0.5918 - acc: 0.6974 - val_loss: 0.5719 - val_acc: 0.7313</p><p>Epoch 00003: val_acc improved from 0.70329 to 0.73129, saving model to aug.weights.003_0.6974_0.7313.h5<br>Epoch 4/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.5609 - acc: 0.7272 - val_loss: 0.5389 - val_acc: 0.7481</p><p>Epoch 00004: val_acc improved from 0.73129 to 0.74808, saving model to aug.weights.004_0.7272_0.7481.h5<br>Epoch 5/1000<br>1172/1172 [==============================] - 277s 236ms/step - loss: 0.5261 - acc: 0.7533 - val_loss: 0.5564 - val_acc: 0.7649</p><p>Epoch 00005: val_acc improved from 0.74808 to 0.76488, saving model to aug.weights.005_0.7534_0.7649.h5<br>Epoch 6/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.4858 - acc: 0.7797 - val_loss: 0.5710 - val_acc: 0.7187</p><p>Epoch 00006: val_acc did not improve from 0.76488<br>Epoch 7/1000<br>1172/1172 [==============================] - 278s 237ms/step - loss: 0.4475 - acc: 0.8004 - val_loss: 0.4229 - val_acc: 0.8085</p><p>Epoch 00007: val_acc improved from 0.76488 to 0.80854, saving model to aug.weights.007_0.8005_0.8085.h5<br>Epoch 8/1000<br>1172/1172 [==============================] - 277s 237ms/step - loss: 0.4182 - acc: 0.8173 - val_loss: 0.3534 - val_acc: 0.8496</p><p>Epoch 00008: val_acc improved from 0.80854 to 0.84965, saving model to aug.weights.008_0.8173_0.8496.h5<br>Epoch 9/1000<br>1172/1172 [==============================] - 278s 237ms/step - loss: 0.3891 - acc: 0.8308 - val_loss: 0.3303 - val_acc: 0.8653</p><p>Epoch 00009: val_acc improved from 0.84965 to 0.86532, saving model to aug.weights.009_0.8309_0.8653.h5<br>Epoch 10/1000<br>1172/1172 [==============================] - 282s 240ms/step - loss: 0.3599 - acc: 0.8506 - val_loss: 0.3535 - val_acc: 0.8578</p><p>Epoch 00010: val_acc did not improve from 0.86532<br>Epoch 11/1000<br>1172/1172 [==============================] - 284s 242ms/step - loss: 0.3280 - acc: 0.8634 - val_loss: 0.3371 - val_acc: 0.8463</p><p>Epoch 00011: val_acc did not improve from 0.86532<br>Epoch 12/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.3135 - acc: 0.8724 - val_loss: 0.2909 - val_acc: 0.8757</p><p>Epoch 00012: val_acc improved from 0.86532 to 0.87572, saving model to aug.weights.012_0.8725_0.8757.h5<br>Epoch 13/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.2972 - acc: 0.8813 - val_loss: 0.3916 - val_acc: 0.8741</p><p>Epoch 00013: val_acc did not improve from 0.87572<br>Epoch 14/1000<br>1172/1172 [==============================] - 284s 242ms/step - loss: 0.2952 - acc: 0.8830 - val_loss: 0.4371 - val_acc: 0.8309</p><p>Epoch 00014: val_acc did not improve from 0.87572<br>Epoch 15/1000<br>1172/1172 [==============================] - 282s 240ms/step - loss: 0.2891 - acc: 0.8856 - val_loss: 0.2576 - val_acc: 0.9023</p><p>Epoch 00015: val_acc improved from 0.87572 to 0.90227, saving model to aug.weights.015_0.8855_0.9023.h5<br>Epoch 16/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.3198 - acc: 0.8787 - val_loss: 0.2797 - val_acc: 0.8832</p><p>Epoch 00016: val_acc did not improve from 0.90227<br>Epoch 17/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.3104 - acc: 0.8823 - val_loss: 0.3470 - val_acc: 0.8916</p><p>Epoch 00017: val_acc did not improve from 0.90227<br>Epoch 18/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.3187 - acc: 0.8800 - val_loss: 0.3794 - val_acc: 0.8821</p><p>Epoch 00018: val_acc did not improve from 0.90227<br>Epoch 19/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.3699 - acc: 0.8669 - val_loss: 0.4585 - val_acc: 0.8895</p><p>Epoch 00019: val_acc did not improve from 0.90227<br>Epoch 20/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.3929 - acc: 0.8582 - val_loss: 0.5537 - val_acc: 0.8581</p><p>Epoch 00020: val_acc did not improve from 0.90227</p><p>Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.<br>Epoch 21/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.2012 - acc: 0.9230 - val_loss: 0.2178 - val_acc: 0.9176</p><p>Epoch 00021: val_acc improved from 0.90227 to 0.91763, saving model to aug.weights.021_0.9230_0.9176.h5<br>Epoch 22/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.1793 - acc: 0.9324 - val_loss: 0.2079 - val_acc: 0.9333</p><p>Epoch 00022: val_acc improved from 0.91763 to 0.93330, saving model to aug.weights.022_0.9324_0.9333.h5<br>Epoch 23/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.1833 - acc: 0.9332 - val_loss: 0.2470 - val_acc: 0.9069</p><p>Epoch 00023: val_acc did not improve from 0.93330<br>Epoch 24/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.1881 - acc: 0.9321 - val_loss: 0.1709 - val_acc: 0.9349</p><p>Epoch 00024: val_acc improved from 0.93330 to 0.93490, saving model to aug.weights.024_0.9320_0.9349.h5<br>Epoch 25/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.1960 - acc: 0.9318 - val_loss: 0.2086 - val_acc: 0.9325</p><p>Epoch 00025: val_acc did not improve from 0.93490<br>Epoch 26/1000<br>1172/1172 [==============================] - 281s 239ms/step - loss: 0.1861 - acc: 0.9300 - val_loss: 0.2057 - val_acc: 0.9307</p><p>Epoch 00026: val_acc did not improve from 0.93490<br>Epoch 27/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.1886 - acc: 0.9305 - val_loss: 0.2576 - val_acc: 0.9376</p><p>Epoch 00027: val_acc improved from 0.93490 to 0.93762, saving model to aug.weights.027_0.9305_0.9376.h5<br>Epoch 28/1000<br>1172/1172 [==============================] - 279s 238ms/step - loss: 0.1985 - acc: 0.9298 - val_loss: 0.3062 - val_acc: 0.8836</p><p>Epoch 00028: val_acc did not improve from 0.93762<br>Epoch 29/1000<br>1172/1172 [==============================] - 281s 240ms/step - loss: 0.2049 - acc: 0.9309 - val_loss: 0.2728 - val_acc: 0.9037</p><p>Epoch 00029: val_acc did not improve from 0.93762<br>Epoch 30/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.2036 - acc: 0.9296 - val_loss: 0.2288 - val_acc: 0.9333</p><p>Epoch 00030: val_acc did not improve from 0.93762<br>Epoch 31/1000<br>1172/1172 [==============================] - 280s 239ms/step - loss: 0.2034 - acc: 0.9267 - val_loss: 0.2206 - val_acc: 0.9171</p><p>Epoch 00031: val_acc did not improve from 0.93762<br>Epoch 32/1000<br>1171/1172 [============================&gt;.] - ETA: 0s - loss: 0.2174 - acc: 0.9257```</p><h3 id="绘制模型训练结果"><a href="#绘制模型训练结果" class="headerlink" title="绘制模型训练结果"></a>绘制模型训练结果<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30325f646174615f617567756d656e746174696f6e2e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/02_data_augumentation.ipynb&repository_id=185701977&repository_type=Repository#绘制模型训练结果" target="_blank" rel="noopener"></a></h3><p>在笔者的实践中，模型在第44轮val_acc达到了0.9440，相对于不带增广的训练，精度有一定的提升。</p><p>In [10]:</p><pre><code>import matplotlib.pyplot as plt# 绘制训练 &amp; 验证的准确率值plt.plot(hist.history[&#39;acc&#39;])plt.plot(hist.history[&#39;val_acc&#39;])plt.title(&#39;Model accuracy&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Val&#39;], loc=&#39;upper left&#39;)plt.show()```![]()加入数据增广后，每一轮的训练时间会变得更长（因为每一批数据都需要进行旋转、缩放等预处理），但是增广后的数据特征更加丰富，有利于提高模型的泛化性和精度，也更加适合样本较少的数据集下的训练。### 扩展[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30325f646174615f617567756d656e746174696f6e2e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/02_data_augumentation.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#扩展)* 尝试调整ImageDataGenerator中的其他参数，并进行组合，查看不同的增广方法对模型训练带来的影响。* 尝试使用较少的图片做数据集，并加入数据增广进行对比。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> modelarts </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习率与优化器</title>
      <link href="/posts/5e9c.html"/>
      <url>/posts/5e9c.html</url>
      
        <content type="html"><![CDATA[<h1 id="学习率与优化器"><a href="#学习率与优化器" class="headerlink" title="学习率与优化器"></a>学习率与优化器<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#学习率与优化器" target="_blank" rel="noopener"></a></h1><p>在本例中，通过比较<strong>同样结构</strong>，<strong>同样数据集</strong>，<strong>相同训练轮数</strong>下<strong>不同优化器</strong>以及<strong>不同初始学习率</strong>下训练的值得出优化器与初始学习率对训练的影响。</p><p>在模型训练过程中涉及到一个关键概念——<strong>学习率</strong>。学习率代表着模型学习的速率，学习率的值越大，损失函数的变化速度越快。较高的学习率大小可以使模型快速收敛，但是可能会导致模型在局部极小值周围震荡。较小的学习率虽然可以让模型在局部极小值周围收敛，但是收敛速度很慢。所以合理的学习率是在底部使用大学习率，在顶部使用小学习率来进行梯度下降。</p><p>对于学习率这样十分重要但是调整难度很大的参数，有几种自适应学习率算法进行学习率的调整。 下面我们采用不同的优化器，对相同的模型结构和数据集训练情况下分别进行训练。我们选择的不同的优化器为：</p><ul><li>rmsprop</li><li>adam</li><li>SGD（stochastic gradient descent）</li></ul><p>这些自适应学习率算法不需要开发者进行学习率的设置而是模型在训练过程中进行学习率的衰减。</p><h4 id="注意：本次实验每一个训练都是只包含5个轮次的训练，每一个5轮次训练大约耗时20分钟。"><a href="#注意：本次实验每一个训练都是只包含5个轮次的训练，每一个5轮次训练大约耗时20分钟。" class="headerlink" title="注意：本次实验每一个训练都是只包含5个轮次的训练，每一个5轮次训练大约耗时20分钟。"></a>注意：本次实验每一个训练都是只包含5个轮次的训练，每一个5轮次训练大约耗时20分钟。<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#注意：本次实验每一个训练都是只包含5个轮次的训练，每一个5轮次训练大约耗时20分钟。" target="_blank" rel="noopener"></a></h4><p>实验之前我们进行keras，keras_applications版本配置以及数据集下载。</p><p>In [1]:</p><pre><code>!pip install --upgrade keras_applications==1.0.6 keras==2.2.4```</code></pre><p>Looking in indexes: <a href="http://repo.myhuaweicloud.com/repository/pypi/simple" target="_blank" rel="noopener">http://repo.myhuaweicloud.com/repository/pypi/simple</a><br>Collecting keras_applications<br>  Downloading <a href="http://repo.myhuaweicloud.com/repository/pypi/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl" target="_blank" rel="noopener">http://repo.myhuaweicloud.com/repository/pypi/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl</a> (50kB)<br>     |████████████████████████████████| 51kB 65.1MB/s eta 0:00:01<br>Collecting keras<br>  Downloading <a href="http://repo.myhuaweicloud.com/repository/pypi/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl" target="_blank" rel="noopener">http://repo.myhuaweicloud.com/repository/pypi/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl</a> (312kB)<br>     |████████████████████████████████| 317kB 63.9MB/s eta 0:00:01<br>Requirement already satisfied, skipping upgrade: h5py in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from keras_applications) (2.8.0)<br>Requirement already satisfied, skipping upgrade: numpy&gt;=1.9.1 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from keras_applications) (1.16.2)<br>Collecting keras-preprocessing&gt;=1.0.5 (from keras)<br>  Downloading <a href="http://repo.myhuaweicloud.com/repository/pypi/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl" target="_blank" rel="noopener">http://repo.myhuaweicloud.com/repository/pypi/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl</a> (41kB)<br>     |████████████████████████████████| 51kB 60.7MB/s eta 0:00:01<br>Requirement already satisfied, skipping upgrade: six&gt;=1.9.0 in /home/ma-user/modelarts-sdk (from keras) (1.12.0)<br>Requirement already satisfied, skipping upgrade: scipy&gt;=0.14 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from keras) (1.0.0)<br>Requirement already satisfied, skipping upgrade: pyyaml in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from keras) (3.12)<br>Installing collected packages: keras-applications, keras-preprocessing, keras<br>  Found existing installation: Keras-Applications 1.0.2<br>    Uninstalling Keras-Applications-1.0.2:<br>      Successfully uninstalled Keras-Applications-1.0.2<br>  Found existing installation: Keras-Preprocessing 1.0.1<br>    Uninstalling Keras-Preprocessing-1.0.1:<br>      Successfully uninstalled Keras-Preprocessing-1.0.1<br>  Found existing installation: Keras 2.2.0<br>    Uninstalling Keras-2.2.0:<br>      Successfully uninstalled Keras-2.2.0<br>Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0</p><pre><code>In \[2\]:</code></pre><p>import osif os.path.exists(‘./data’) == False:<br>    from modelarts.session import Session<br>    session = Session()</p><pre><code>if session.region_name == &#39;cn-north-1&#39;:    bucket_path=&quot;modelarts-labs/end2end/image_recognition/dog_and_cat_25000.tar.gz&quot;elif session.region_name == &#39;cn-north-4&#39;:    bucket_path=&quot;modelarts-labs-bj4/end2end/image_recognition/dog_and_cat_25000.tar.gz&quot;else:    print(&quot;请更换地区到北京一或北京四&quot;)session.download_data(    bucket_path=bucket_path,    path=&quot;./dog_and_cat_25000.tar.gz&quot;)# 使用tar命令解压资源包!tar xf ./dog_and_cat_25000.tar.gz# 清理压缩包!rm -f ./dog_and_cat_25000.tar.gz```</code></pre><pre><code>Successfully download file modelarts-labs/end2end/image_recognition/dog_and_cat_25000.tar.gz from OBS to local ./dog_and_cat_25000.tar.gz</code></pre><h2 id="引入相关的包"><a href="#引入相关的包" class="headerlink" title="引入相关的包"></a>引入相关的包<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#引入相关的包" target="_blank" rel="noopener"></a></h2><p>In [3]:</p><pre><code>from keras.applications.vgg16 import VGG16from keras.preprocessing import imagefrom keras.applications.resnet50 import preprocess_input, decode_predictionsimport numpy as npfrom keras.applications.mobilenetv2 import MobileNetV2from keras.preprocessing import imagefrom keras.models import Modelfrom keras.layers import Dense, GlobalAveragePooling2Dfrom keras import backend as Kfrom keras.models import load_modelfrom keras.preprocessing.image import ImageDataGeneratorimport osfrom PIL import Image```</code></pre><p>Using TensorFlow backend.</p><pre><code>## 加载数据[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#加载数据)In \[4\]:</code></pre><p>def load_data():<br>    dirname = “./data”<br>    path = “./data”</p><pre><code>num_train_samples = 25000x_train = np.empty((num_train_samples, 224,224,3), dtype=&#39;uint8&#39;)y_train = np.empty((num_train_samples,1), dtype=&#39;uint8&#39;)index = 0for file in os.listdir(&quot;./data&quot;):    image = Image.open(os.path.join(dirname,file)).resize((224,224))    image = np.array(image)    x_train[index,:,:,:] = image    if &quot;cat&quot; in file:        y_train[index,0] =1    elif &quot;dog&quot; in file:        y_train[index,0] =0    index += 1return (x_train, y_train)```</code></pre><p>In [5]:</p><pre><code>(x_train, y_train) = load_data()print(x_train.shape)print(y_train.shape)```</code></pre><p>(25000, 224, 224, 3)<br>(25000, 1)</p><pre><code>## 数据处理[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#数据处理)In \[6\]:</code></pre><p>from keras.utils import np_utilsdef process_data(x_train,y_train):<br>    x_train = x_train.astype(np.float32)<br>    x_train /= 255<br>    n_classes = 2<br>    y_train = np_utils.to_categorical(y_train, n_classes)<br>    return x_train,y_train```<br>In [7]:</p><pre><code>x_train,y_train= process_data(x_train,y_train)print(x_train.shape)print(y_train.shape)```</code></pre><p>(25000, 224, 224, 3)<br>(25000, 2)</p><pre><code>## 构建模型[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#构建模型)In \[8\]:</code></pre><p>def build_model(base_model):<br>    x = base_model.output<br>    x = GlobalAveragePooling2D()(x)<br>    predictions = Dense(2, activation=’softmax’)(x)<br>    model = Model(inputs=base_model.input, outputs=predictions)<br>    print(type(model))<br>    return model```<br>In [9]:</p><pre><code>base_model = VGG16(weights=None, include_top=False)```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Colocations handled automatically by placer.</p><pre><code>In \[10\]:</code></pre><p>model = build_model(base_model)model.summary()```</p><pre><code>&lt;class &#39;keras.engine.training.Model&#39;&gt;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, None, None, 3)     0         _________________________________________________________________block1_conv1 (Conv2D)        (None, None, None, 64)    1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, None, None, 64)    36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, None, None, 64)    0         _________________________________________________________________block2_conv1 (Conv2D)        (None, None, None, 128)   73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, None, None, 128)   147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, None, None, 128)   0         _________________________________________________________________block3_conv1 (Conv2D)        (None, None, None, 256)   295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, None, None, 256)   0         _________________________________________________________________block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 2)                 1026      =================================================================Total params: 14,715,714Trainable params: 14,715,714Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="定义优化器与训练"><a href="#定义优化器与训练" class="headerlink" title="定义优化器与训练"></a>定义优化器与训练<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#定义优化器与训练" target="_blank" rel="noopener"></a></h2><h3 id="rmsprop"><a href="#rmsprop" class="headerlink" title="rmsprop"></a>rmsprop<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#rmsprop" target="_blank" rel="noopener"></a></h3><p>在下面的测试中，我们使用优化器为<strong>rmsprop</strong>，训练轮数为5，可以看到训练初期的模型指数变化情况。</p><p>In [11]:</p><pre><code>import keras opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)model.compile(loss=&#39;categorical_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```In \[12\]:</code></pre><p>from keras.callbacks import ModelCheckpoint, EarlyStoppinges = EarlyStopping(monitor=’val_acc’, baseline=0.9, patience=30, verbose=1, mode=’auto’)callbacks = [es]```<br>开始训练</p><p>In [13]:</p><pre><code>history_rmsprop = model.fit(x=x_train,                   y=y_train,                   batch_size=32,                   epochs=5,                   verbose=1,                   callbacks=callbacks,                   validation_split=0.25,                   shuffle=True,                   initial_epoch=0,                  )```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Use tf.cast instead.</p><pre><code></code></pre><p>Train on 18750 samples, validate on 6250 samples<br>Epoch 1/5<br>18750/18750 [==============================] - 168s 9ms/step - loss: 0.6715 - acc: 0.5895 - val_loss: 0.6318 - val_acc: 0.6414<br>Epoch 2/5<br>18750/18750 [==============================] - 153s 8ms/step - loss: 0.6241 - acc: 0.6555 - val_loss: 0.7510 - val_acc: 0.5550<br>Epoch 3/5<br>18750/18750 [==============================] - 153s 8ms/step - loss: 0.5826 - acc: 0.7009 - val_loss: 0.5227 - val_acc: 0.7523<br>Epoch 4/5<br>18750/18750 [==============================] - 153s 8ms/step - loss: 0.5320 - acc: 0.7400 - val_loss: 0.4935 - val_acc: 0.7645<br>Epoch 5/5<br>18750/18750 [==============================] - 154s 8ms/step - loss: 0.4848 - acc: 0.7730 - val_loss: 0.4404 - val_acc: 0.8040</p><pre><code>In \[14\]:</code></pre><p>import matplotlib.pyplot as plt</p><h1 id="绘制训练-amp-验证的准确率值plt-plot-history-rmsprop-history-‘acc’-plt-plot-history-rmsprop-history-‘val-acc’-plt-title-‘Model-accuracy’-plt-ylabel-‘Accuracy’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的准确率值plt-plot-history-rmsprop-history-‘acc’-plt-plot-history-rmsprop-history-‘val-acc’-plt-title-‘Model-accuracy’-plt-ylabel-‘Accuracy’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的准确率值plt.plot(history_rmsprop.history[‘acc’])plt.plot(history_rmsprop.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的准确率值plt.plot(history_rmsprop.history[‘acc’])plt.plot(history_rmsprop.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><pre><code>&lt;Figure size 640x480 with 1 Axes&gt;```In \[15\]:</code></pre><h1 id="绘制训练-amp-验证的损失值plt-plot-history-rmsprop-history-‘loss’-plt-plot-history-rmsprop-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的损失值plt-plot-history-rmsprop-history-‘loss’-plt-plot-history-rmsprop-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的损失值plt.plot(history_rmsprop.history[‘loss’])plt.plot(history_rmsprop.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的损失值plt.plot(history_rmsprop.history[‘loss’])plt.plot(history_rmsprop.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><p><img src="" alt=""></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#Adam" target="_blank" rel="noopener"></a></h3><p>在下面的例子中，加载新的模型，使用<strong>优化器Adam</strong>，训练轮数为5，可以看到训练初期的模型指数变化情况。</p><p>In [16]:</p><pre><code>base_model = VGG16(weights=None, include_top=False)model_adam = build_model(base_model)opt = keras.optimizers.Adam(lr=0.0001, decay=1e-6)model_adam.compile(loss=&#39;categorical_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```</code></pre><p>&lt;class ‘keras.engine.training.Model’&gt;</p><pre><code>开始训练In \[17\]:</code></pre><p>history_adam = model_adam.fit(x=x_train,<br>                  y=y_train,<br>                  batch_size=32,<br>                  epochs=5,<br>                  verbose=1,<br>                  callbacks=callbacks,<br>                   validation_split=0.25,<br>                  shuffle=True,<br>                  initial_epoch=0<br>                 )```</p><pre><code>Train on 18750 samples, validate on 6250 samplesEpoch 1/518750/18750 [==============================] - 157s 8ms/step - loss: 0.6928 - acc: 0.5049 - val_loss: 0.6929 - val_acc: 0.5013Epoch 2/518750/18750 [==============================] - 152s 8ms/step - loss: 0.6677 - acc: 0.5748 - val_loss: 0.6268 - val_acc: 0.6442Epoch 3/518750/18750 [==============================] - 151s 8ms/step - loss: 0.6080 - acc: 0.6685 - val_loss: 0.6029 - val_acc: 0.6670Epoch 4/518750/18750 [==============================] - 153s 8ms/step - loss: 0.5671 - acc: 0.7067 - val_loss: 0.5263 - val_acc: 0.7410Epoch 5/518750/18750 [==============================] - 151s 8ms/step - loss: 0.5306 - acc: 0.7381 - val_loss: 0.4824 - val_acc: 0.7754</code></pre><p>In [18]:</p><pre><code>import matplotlib.pyplot as plt# 绘制训练 &amp; 验证的准确率值plt.plot(history_adam.history[&#39;acc&#39;])plt.plot(history_adam.history[&#39;val_acc&#39;])plt.title(&#39;Model accuracy&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()In \[19\]:</code></pre><h1 id="绘制训练-amp-验证的损失值plt-plot-history-adam-history-‘loss’-plt-plot-history-adam-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的损失值plt-plot-history-adam-history-‘loss’-plt-plot-history-adam-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的损失值plt.plot(history_adam.history[‘loss’])plt.plot(history_adam.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的损失值plt.plot(history_adam.history[‘loss’])plt.plot(history_adam.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><p><img src="" alt=""></p><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#SGD" target="_blank" rel="noopener"></a></h3><p>在下面的例子中，加载新的模型，使用<strong>优化器SGD</strong>，训练轮数为5，可以看到训练初期的模型指数变化情况。可以看到SGD作为随机梯度下降模型，在训练初期较少的轮数下，表现并不稳定。</p><p>In [20]:</p><pre><code>base_model = VGG16(weights=None, include_top=False)model_sgd = build_model(base_model)opt = keras.optimizers.SGD(lr=0.0001, decay=1e-6)model_sgd.compile(loss=&#39;categorical_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```</code></pre><p>&lt;class ‘keras.engine.training.Model’&gt;</p><pre><code>开始训练In \[21\]:</code></pre><p>history_sgd = model_sgd.fit(x=x_train,<br>                  y=y_train,<br>                  batch_size=32,<br>                  epochs=5,<br>                  verbose=1,<br>                  callbacks=callbacks,<br>                  validation_split=0.25,<br>                  shuffle=True,<br>                  initial_epoch=0,<br>                 )```</p><pre><code>Train on 18750 samples, validate on 6250 samplesEpoch 1/518750/18750 [==============================] - 151s 8ms/step - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5056Epoch 2/518750/18750 [==============================] - 149s 8ms/step - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5053Epoch 3/518750/18750 [==============================] - 148s 8ms/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.5027Epoch 4/518750/18750 [==============================] - 149s 8ms/step - loss: 0.6932 - acc: 0.4789 - val_loss: 0.6932 - val_acc: 0.4768Epoch 5/518750/18750 [==============================] - 149s 8ms/step - loss: 0.6932 - acc: 0.4602 - val_loss: 0.6932 - val_acc: 0.4510</code></pre><p>In [22]:</p><pre><code>import matplotlib.pyplot as plt# 绘制训练 &amp; 验证的准确率值plt.plot(history_sgd.history[&#39;acc&#39;])plt.plot(history_sgd.history[&#39;val_acc&#39;])plt.title(&#39;Model accuracy&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()In \[23\]:</code></pre><h1 id="绘制训练-amp-验证的损失值plt-plot-history-sgd-history-‘loss’-plt-plot-history-sgd-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的损失值plt-plot-history-sgd-history-‘loss’-plt-plot-history-sgd-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的损失值plt.plot(history_sgd.history[‘loss’])plt.plot(history_sgd.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的损失值plt.plot(history_sgd.history[‘loss’])plt.plot(history_sgd.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><p><img src="" alt=""><br><strong>下面的训练中学习率的初始赋值与上一次训练相比由0.0001变为0.001</strong> 下面的训练结果并不出色，对比我们之前的Adam优化器，这次的训练几乎没有收敛。因为过大的学习率，导致模型在局部最优附近震荡，无法获得好的结果。</p><p>In [24]:</p><pre><code>base_model = VGG16(weights=None, include_top=False)model_large_lr = build_model(base_model)opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)model_large_lr.compile(loss=&#39;categorical_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```</code></pre><p>&lt;class ‘keras.engine.training.Model’&gt;</p><pre><code>开始训练In \[25\]:</code></pre><p>history_large_lr = model_large_lr.fit(x=x_train,<br>                  y=y_train,<br>                  batch_size=32,<br>                  epochs=5,<br>                  verbose=1,<br>                  callbacks=callbacks,<br>                  validation_split=0.25,<br>                  shuffle=True,<br>                  initial_epoch=0,<br>                 )```</p><pre><code>Train on 18750 samples, validate on 6250 samplesEpoch 1/518750/18750 [==============================] - 152s 8ms/step - loss: 0.6939 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.4944Epoch 2/518750/18750 [==============================] - 151s 8ms/step - loss: 0.6932 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.4944Epoch 3/518750/18750 [==============================] - 150s 8ms/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5056Epoch 4/518750/18750 [==============================] - 151s 8ms/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4944Epoch 5/518750/18750 [==============================] - 150s 8ms/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5056</code></pre><p>In [26]:</p><pre><code># 绘制训练 &amp; 验证的准确率值plt.plot(history_large_lr.history[&#39;acc&#39;])plt.plot(history_large_lr.history[&#39;val_acc&#39;])plt.title(&#39;Model accuracy&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()In \[27\]:</code></pre><p>plt.plot(history_large_lr.history[‘loss’])plt.plot(history_large_lr.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```<br><img src="" alt=""></p><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30315f6c725f6f70742e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/01_lr_opt.ipynb&repository_id=185701977&repository_type=Repository#思考" target="_blank" rel="noopener"></a></h1><p>以上的优化器对比都是在训练初期轮数较少的情况下进行，但是在轮数较多或者出现鞍点等情况时，不同的优化器都有不一样的表现。可以尝试用更多的轮数对比不同的优化器，重新审视各个模型在训练各个阶段的表现。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> modelarts </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>训练轮数与callbacks</title>
      <link href="/posts/c9b.html"/>
      <url>/posts/c9b.html</url>
      
        <content type="html"><![CDATA[<h1 id="训练轮数与callbacks"><a href="#训练轮数与callbacks" class="headerlink" title="训练轮数与callbacks"></a>训练轮数与callbacks<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#训练轮数与callbacks" target="_blank" rel="noopener"></a></h1><p>在本例中，通过比较<strong>同样结构</strong>，<strong>同样优化器</strong>，<strong>同样数据集</strong>下<strong>不同epoch</strong>以及<strong>不同callbacks方法</strong>下训练的值得出训练轮数与callbacks方法对训练的影响。</p><p>训练轮数决定了训练达到的程度，在接下来的实验中，我们尝试了逐步增加训练轮数，观察模型在不同阶段的收敛情况。以5轮次为单位进行观察。</p><h4 id="注意：-每一个5轮次训练大约耗时20分钟。"><a href="#注意：-每一个5轮次训练大约耗时20分钟。" class="headerlink" title="注意： 每一个5轮次训练大约耗时20分钟。"></a>注意： 每一个5轮次训练大约耗时20分钟。<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#注意：-每一个5轮次训练大约耗时20分钟。" target="_blank" rel="noopener"></a></h4><p>callbacks方法中，我们将介绍：</p><ul><li>ModelCheckpoint</li><li>EarlyStopping</li><li>ReduceLROnPlateau</li></ul><p>实验之前我们进行keras，keras_applications版本配置以及数据集下载。</p><p>In [1]:</p><pre><code>!pip install --upgrade keras_applications==1.0.6 keras==2.2.4```In \[2\]:</code></pre><p>import osif os.path.exists(‘./data’) == False:<br>    from modelarts.session import Session<br>    session = Session()</p><pre><code>if session.region_name == &#39;cn-north-1&#39;:    bucket_path=&quot;modelarts-labs/end2end/image_recognition/dog_and_cat_25000.tar.gz&quot;elif session.region_name == &#39;cn-north-4&#39;:    bucket_path=&quot;modelarts-labs-bj4/end2end/image_recognition/dog_and_cat_25000.tar.gz&quot;else:    print(&quot;请更换地区到北京一或北京四&quot;)session.download_data(    bucket_path=bucket_path,    path=&quot;./dog_and_cat_25000.tar.gz&quot;)# 使用tar命令解压资源包!tar xf ./dog_and_cat_25000.tar.gz# 清理压缩包!rm -f ./dog_and_cat_25000.tar.gz</code></pre><pre><code></code></pre><p>Successfully download file modelarts-labs/end2end/image_recognition/dog_and_cat_25000.tar.gz from OBS to local ./dog_and_cat_25000.tar.gz</p><pre><code>In \[3\]:</code></pre><p>!mkdir model</p><pre><code>## 引入相关的包[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#引入相关的包)In \[4\]:</code></pre><p>from keras.applications.vgg16 import VGG16from keras.preprocessing import imageimport numpy as np</p><p>from keras.preprocessing import imagefrom keras.models import Modelfrom keras.layers import Dense, GlobalAveragePooling2Dfrom keras import backend as Kfrom keras.models import load_model</p><p>from keras.preprocessing.image import ImageDataGenerator```</p><pre><code>Using TensorFlow backend.</code></pre><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#读取数据" target="_blank" rel="noopener"></a></h2><p>In [5]:</p><pre><code>import osfrom PIL import Imagedef load_data():    dirname = &quot;./data&quot;    path = &quot;./data&quot;    num_train_samples = 25000    x_train = np.empty((num_train_samples, 224,224,3), dtype=&#39;uint8&#39;)    y_train = np.empty((num_train_samples,1), dtype=&#39;uint8&#39;)    index = 0    for file in os.listdir(&quot;./data&quot;):        image = Image.open(os.path.join(dirname,file)).resize((224,224))        image = np.array(image)        x_train[index,:,:,:] = image        if &quot;cat&quot; in file:            y_train[index,0] =1        elif &quot;dog&quot; in file:            y_train[index,0] =0        index += 1    return (x_train, y_train)```In \[6\]:</code></pre><p>(x_train, y_train) = load_data()```<br>In [7]:</p><pre><code>print(x_train.shape)print(y_train.shape)```</code></pre><p>(25000, 224, 224, 3)<br>(25000, 1)</p><pre><code>## 数据处理[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#数据处理)In \[8\]:</code></pre><p>from keras.utils import np_utilsdef process_data(x_train,y_train):<br>    x_train = x_train.astype(np.float32)<br>    x_train /= 255<br>    n_classes = 2<br>    y_train = np_utils.to_categorical(y_train, n_classes)<br>    return x_train,y_train```<br>In [9]:</p><pre><code>x_train,y_train= process_data(x_train,y_train)print(x_train.shape)print(y_train.shape)```</code></pre><p>(25000, 224, 224, 3)<br>(25000, 2)</p><pre><code>## 构建模型[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#构建模型)In \[10\]:</code></pre><p>def build_model(base_model):<br>    x = base_model.output<br>    x = GlobalAveragePooling2D()(x)<br>    predictions = Dense(2, activation=’softmax’)(x)<br>    model = Model(inputs=base_model.input, outputs=predictions)<br>    print(type(model))<br>    return model```<br>In [11]:</p><pre><code>base_model = VGG16(weights=None, include_top=False)```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Colocations handled automatically by placer.</p><pre><code>In \[12\]:</code></pre><p>model = build_model(base_model)model.summary()```</p><pre><code>&lt;class &#39;keras.engine.training.Model&#39;&gt;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, None, None, 3)     0         _________________________________________________________________block1_conv1 (Conv2D)        (None, None, None, 64)    1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, None, None, 64)    36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, None, None, 64)    0         _________________________________________________________________block2_conv1 (Conv2D)        (None, None, None, 128)   73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, None, None, 128)   147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, None, None, 128)   0         _________________________________________________________________block3_conv1 (Conv2D)        (None, None, None, 256)   295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, None, None, 256)   0         _________________________________________________________________block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 2)                 1026      =================================================================Total params: 14,715,714Trainable params: 14,715,714Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#模型训练" target="_blank" rel="noopener"></a></h2><h3 id="5-epoch训练"><a href="#5-epoch训练" class="headerlink" title="5 epoch训练"></a>5 epoch训练<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#5-epoch训练" target="_blank" rel="noopener"></a></h3><p>In [13]:</p><pre><code>import keras opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)model.compile(loss=&#39;binary_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```In \[14\]:</code></pre><p>from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateaues = EarlyStopping(monitor=’val_acc’, min_delta=0.001, patience=5, verbose=1, mode=’auto’)cp = ModelCheckpoint(filepath=”./model/ckp_vgg16_dog_and_cat.h5”, monitor=”val_acc”, verbose=1, save_best_only=True, mode=”auto”, period=1)lr = ReduceLROnPlateau(monitor=”val_acc”, factor=0.1, patience=3, verbose=1, mode=”auto”, min_lr=0)callbacks = [es,cp,lr]```<br>In [15]:</p><pre><code>history = model.fit(x=x_train,                   y=y_train,                   batch_size=16,                   epochs=5,                   verbose=1,                   callbacks=callbacks,                   validation_split=0.25,                   shuffle=True,                   initial_epoch=0,                  )```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Use tf.cast instead.<br>Train on 18750 samples, validate on 6250 samples<br>Epoch 1/5<br>18750/18750 [==============================] - 176s 9ms/step - loss: 0.6664 - acc: 0.5893 - val_loss: 0.6203 - val_acc: 0.6547</p><p>Epoch 00001: val_acc improved from -inf to 0.65472, saving model to ./model/ckp_vgg16_dog_and_cat.h5<br>Epoch 2/5<br>18750/18750 [==============================] - 164s 9ms/step - loss: 0.6031 - acc: 0.6736 - val_loss: 0.5685 - val_acc: 0.7018</p><p>Epoch 00002: val_acc improved from 0.65472 to 0.70176, saving model to ./model/ckp_vgg16_dog_and_cat.h5<br>Epoch 3/5<br>18750/18750 [==============================] - 164s 9ms/step - loss: 0.5573 - acc: 0.7200 - val_loss: 0.6744 - val_acc: 0.7107</p><p>Epoch 00003: val_acc improved from 0.70176 to 0.71072, saving model to ./model/ckp_vgg16_dog_and_cat.h5<br>Epoch 4/5<br>18750/18750 [==============================] - 163s 9ms/step - loss: 0.4789 - acc: 0.7725 - val_loss: 0.4174 - val_acc: 0.8075</p><p>Epoch 00004: val_acc improved from 0.71072 to 0.80752, saving model to ./model/ckp_vgg16_dog_and_cat.h5<br>Epoch 5/5<br>18750/18750 [==============================] - 163s 9ms/step - loss: 0.3898 - acc: 0.8216 - val_loss: 0.3494 - val_acc: 0.8622</p><p>Epoch 00005: val_acc improved from 0.80752 to 0.86224, saving model to ./model/ckp_vgg16_dog_and_cat.h5</p><pre><code>In \[27\]:</code></pre><p>import matplotlib.pyplot as plt</p><h1 id="绘制训练-amp-验证的准确率值plt-plot-history-history-‘acc’-plt-plot-history-history-‘val-acc’-plt-title-‘Model-accuracy’-plt-ylabel-‘Accuracy’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的准确率值plt-plot-history-history-‘acc’-plt-plot-history-history-‘val-acc’-plt-title-‘Model-accuracy’-plt-ylabel-‘Accuracy’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的准确率值plt.plot(history.history[‘acc’])plt.plot(history.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的准确率值plt.plot(history.history[‘acc’])plt.plot(history.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><p><img src="" alt=""><br>In [17]:</p><pre><code># 绘制训练 &amp; 验证的损失值plt.plot(history.history[&#39;loss&#39;])plt.plot(history.history[&#39;val_loss&#39;])plt.title(&#39;Model loss&#39;)plt.ylabel(&#39;Loss&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()### 10 epoch 训练[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#10-epoch-训练)**在下面的训练中，epoch将再训练5轮**#### Epoch[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#Epoch)在Epoch数值较小时，出现了上一个训练中出现的欠拟合情况，模型没有很好收敛训练便结束了。在接下来的训练中，我们提高了epoch的值，通过训练结果我们可以看到模型逐渐收敛。但是epoch的值并非越大越好，过大的epoch值可能会导致过拟合现象。![]()epoch的值没有具体的公式进行计算，需要根据经验和具体的情况进行制定。更多的epoch训练大家可以在拓展中进行尝试。#### ModelCheckpoint[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#ModelCheckpoint)在模型训练过程中，ModelCheckpoint将出现的最好的权重进行保存。 在下面的训练中，每一次出现更好的模型，epoch完成后都进行了保存。#### ReduceLROnPlateau[](https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#ReduceLROnPlateau)学习率衰减方法，在指定epoch数量结束后检测指标时候有提升，如果提升较小，便进行学习率衰减。在接下来的训练中，通过方法，学习率进行了多次调整，调整之后的模型的指标有所提升。In \[18\]:</code></pre><p>history_more_steps = model.fit(x=x_train,<br>                  y=y_train,<br>                  batch_size=16,<br>                  epochs=5,<br>                  verbose=1,<br>                  callbacks=callbacks,<br>                  validation_split=0.25,<br>                  shuffle=True,<br>                  initial_epoch=0,<br>                 )```</p><pre><code>Train on 18750 samples, validate on 6250 samplesEpoch 1/518750/18750 [==============================] - 164s 9ms/step - loss: 0.3249 - acc: 0.8578 - val_loss: 0.6155 - val_acc: 0.7531Epoch 00001: val_acc did not improve from 0.86224Epoch 2/518750/18750 [==============================] - 166s 9ms/step - loss: 0.2831 - acc: 0.8798 - val_loss: 0.2328 - val_acc: 0.9030Epoch 00002: val_acc improved from 0.86224 to 0.90304, saving model to ./model/ckp_vgg16_dog_and_cat.h5Epoch 3/518750/18750 [==============================] - 165s 9ms/step - loss: 0.2471 - acc: 0.8962 - val_loss: 0.2751 - val_acc: 0.8858Epoch 00003: val_acc did not improve from 0.90304Epoch 4/518750/18750 [==============================] - 164s 9ms/step - loss: 0.2215 - acc: 0.9078 - val_loss: 0.1983 - val_acc: 0.9179Epoch 00004: val_acc improved from 0.90304 to 0.91792, saving model to ./model/ckp_vgg16_dog_and_cat.h5Epoch 5/518750/18750 [==============================] - 164s 9ms/step - loss: 0.1954 - acc: 0.9203 - val_loss: 0.2674 - val_acc: 0.9032Epoch 00005: val_acc did not improve from 0.91792</code></pre><p>In [19]:</p><pre><code>import matplotlib.pyplot as plt# 绘制训练 &amp; 验证的准确率值plt.plot(history_more_steps.history[&#39;acc&#39;])plt.plot(history_more_steps.history[&#39;val_acc&#39;])plt.title(&#39;Model accuracy&#39;)plt.ylabel(&#39;Accuracy&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()In \[20\]:</code></pre><h1 id="绘制训练-amp-验证的损失值plt-plot-history-more-steps-history-‘loss’-plt-plot-history-more-steps-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的损失值plt-plot-history-more-steps-history-‘loss’-plt-plot-history-more-steps-history-‘val-loss’-plt-title-‘Model-loss’-plt-ylabel-‘Loss’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的损失值plt.plot(history_more_steps.history[‘loss’])plt.plot(history_more_steps.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的损失值plt.plot(history_more_steps.history[‘loss’])plt.plot(history_more_steps.history[‘val_loss’])plt.title(‘Model loss’)plt.ylabel(‘Loss’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><p><img src="" alt=""></p><h4 id="EarlyStopping-早停法"><a href="#EarlyStopping-早停法" class="headerlink" title="EarlyStopping 早停法"></a>EarlyStopping 早停法<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#EarlyStopping-早停法" target="_blank" rel="noopener"></a></h4><p>根据上一个训练可以看到，在训练的后期模型的loss和acc数值已经稳定，这时候继续训练没有对数值产生影响还有可能产生过拟合情况，所以需要及时将模型训练停止。EarlyStopping（早停法）检测模型的某一项指标，如果在指定步数中指标没有提升，则将模型训练停止。</p><p>在下面的训练中如果训练没有显著提升，则停止训练。</p><h4 id="注意：可以修改早停参数，进行模型训练停止。当前模型训练上限为20轮次，如果模型val-acc没有0-001提升则停止。"><a href="#注意：可以修改早停参数，进行模型训练停止。当前模型训练上限为20轮次，如果模型val-acc没有0-001提升则停止。" class="headerlink" title="注意：可以修改早停参数，进行模型训练停止。当前模型训练上限为20轮次，如果模型val_acc没有0.001提升则停止。"></a>注意：可以修改早停参数，进行模型训练停止。当前模型训练上限为20轮次，如果模型val_acc没有0.001提升则停止。<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#注意：可以修改早停参数，进行模型训练停止。当前模型训练上限为20轮次，如果模型val_acc没有0.001提升则停止。" target="_blank" rel="noopener"></a></h4><p>In [23]:</p><pre><code>es = EarlyStopping(monitor=&#39;val_acc&#39;, min_delta=0.001, patience=1, verbose=1, mode=&#39;auto&#39;)cp = ModelCheckpoint(filepath=&quot;./model/ckp_vgg16_dog_and_cat.h5&quot;, monitor=&quot;val_acc&quot;, verbose=1, save_best_only=True, mode=&quot;auto&quot;, period=1)lr = ReduceLROnPlateau(monitor=&quot;val_acc&quot;, factor=0.1, patience=3, verbose=1, mode=&quot;auto&quot;, min_lr=0)callbacks = [es,cp,lr]```In \[24\]:</code></pre><p>history_steps_15 = model.fit(x=x_train,<br>                  y=y_train,<br>                  batch_size=16,<br>                  epochs=20,<br>                  verbose=1,<br>                  callbacks=callbacks,<br>                  validation_split=0.25,<br>                  shuffle=True,<br>                  initial_epoch=0,<br>                 )```</p><pre><code>Train on 18750 samples, validate on 6250 samplesEpoch 1/2018750/18750 [==============================] - 159s 8ms/step - loss: 0.1350 - acc: 0.9506 - val_loss: 0.1675 - val_acc: 0.9357Epoch 00001: val_acc improved from -inf to 0.93568, saving model to ./model/ckp_vgg16_dog_and_cat.h5Epoch 2/2018750/18750 [==============================] - 161s 9ms/step - loss: 0.1276 - acc: 0.9523 - val_loss: 0.1752 - val_acc: 0.9338Epoch 00002: val_acc did not improve from 0.93568Epoch 00002: early stopping</code></pre><p>In [25]:</p><pre><code># 绘制训练 &amp; 验证的损失值plt.plot(history_steps_15.history[&#39;loss&#39;])plt.plot(history_steps_15.history[&#39;val_loss&#39;])plt.title(&#39;Model loss&#39;)plt.ylabel(&#39;Loss&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()In \[26\]:</code></pre><h1 id="绘制训练-amp-验证的准确率值plt-plot-history-steps-15-history-‘acc’-plt-plot-history-steps-15-history-‘val-acc’-plt-title-‘Model-accuracy’-plt-ylabel-‘Accuracy’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show"><a href="#绘制训练-amp-验证的准确率值plt-plot-history-steps-15-history-‘acc’-plt-plot-history-steps-15-history-‘val-acc’-plt-title-‘Model-accuracy’-plt-ylabel-‘Accuracy’-plt-xlabel-‘Epoch’-plt-legend-‘Train’-‘Test’-loc-’upper-left’-plt-show" class="headerlink" title="绘制训练 &amp; 验证的准确率值plt.plot(history_steps_15.history[‘acc’])plt.plot(history_steps_15.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```"></a>绘制训练 &amp; 验证的准确率值plt.plot(history_steps_15.history[‘acc’])plt.plot(history_steps_15.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</h1><p><img src="" alt=""></p><h1 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展<a href="https://nbviewer.jupyter.org/github/huaweicloud/ModelArts-Lab/blob/master/notebook/DL_image_hyperparameter_tuning/00_epoch_callbacks.ipynb#拓展" target="_blank" rel="noopener"></a></h1><ul><li>可以尝试自己定义学习率衰减规律。使用方法，自己定义学习率衰减。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> modelarts </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习第一天（pytorch学习笔记）</title>
      <link href="/posts/6080.html"/>
      <url>/posts/6080.html</url>
      
        <content type="html"><![CDATA[<h1 id="入坑（2020-4-29）"><a href="#入坑（2020-4-29）" class="headerlink" title="入坑（2020.4.29）"></a>入坑（2020.4.29）</h1><p>喜欢深度学习，喜欢python。<br>看起来torch挺好的。</p><h1 id="torch环境准备"><a href="#torch环境准备" class="headerlink" title="torch环境准备"></a>torch环境准备</h1><ul><li>torch  (1.5.0)</li><li>torchvision  (0.6.0)</li><li>cuda  (10.2)<br><img src="https://img-blog.csdnimg.cn/20200429183137908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTk3MzYzMA==,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>输入指令执行代码,速度不行的话复制下载链接迅雷打开，可能有惊喜<img src="https://img-blog.csdnimg.cn/20200429183453171.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTk3MzYzMA==,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>环境配好后，执行<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> pythontorch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>这时应该返回True.</li></ul><h1 id="一次比赛的代码"><a href="#一次比赛的代码" class="headerlink" title="一次比赛的代码"></a>一次比赛的代码</h1><p>不懂的话直接当普通代码执行，当然本人还是纯小白，代码是copy来的，一次比赛的baseline代码</p><h4 id="加载pytorch框架下的依赖项"><a href="#加载pytorch框架下的依赖项" class="headerlink" title="加载pytorch框架下的依赖项"></a>加载pytorch框架下的依赖项</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> print_function<span class="token punctuation">,</span> division<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> lr_scheduler<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token keyword">import</span> torchvision<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> models<span class="token punctuation">,</span> transforms<span class="token keyword">import</span> time<span class="token keyword">import</span> os</code></pre><h4 id="加载数据集，并分为训练集和测试集"><a href="#加载数据集，并分为训练集和测试集" class="headerlink" title="加载数据集，并分为训练集和测试集"></a>加载数据集，并分为训练集和测试集</h4><pre class=" language-python"><code class="language-python">dataTrans <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>            transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span>data_dir <span class="token operator">=</span> <span class="token string">'./images'</span>all_image_datasets <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> dataTrans<span class="token punctuation">)</span>trainsize <span class="token operator">=</span> int<span class="token punctuation">(</span><span class="token number">0.8</span><span class="token operator">*</span>len<span class="token punctuation">(</span>all_image_datasets<span class="token punctuation">)</span><span class="token punctuation">)</span>testsize <span class="token operator">=</span> len<span class="token punctuation">(</span>all_image_datasets<span class="token punctuation">)</span> <span class="token operator">-</span> trainsizetrain_dataset<span class="token punctuation">,</span> test_dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>all_image_datasets<span class="token punctuation">,</span><span class="token punctuation">[</span>trainsize<span class="token punctuation">,</span>testsize<span class="token punctuation">]</span><span class="token punctuation">)</span>image_datasets <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'train'</span><span class="token punctuation">:</span>train_dataset<span class="token punctuation">,</span><span class="token string">'val'</span><span class="token punctuation">:</span>test_dataset<span class="token punctuation">}</span>dataloders <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>image_datasets<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                 batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>                                                 shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                                                 num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>dataset_sizes <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> len<span class="token punctuation">(</span>image_datasets<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true"># use gpu or not</span>use_gpu <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_model</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> lossfunc<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> scheduler<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    best_model_wts <span class="token operator">=</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>    best_acc <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch {}/{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> num_epochs <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-'</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Each epoch has a training and validation phase</span>        <span class="token keyword">for</span> phase <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Set model to training mode</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Set model to evaluate mode</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span>            running_corrects <span class="token operator">=</span> <span class="token number">0.0</span>            <span class="token comment" spellcheck="true"># Iterate over data.</span>            <span class="token keyword">for</span> data <span class="token keyword">in</span> dataloders<span class="token punctuation">[</span>phase<span class="token punctuation">]</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># get the inputs</span>                inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data                <span class="token comment" spellcheck="true"># wrap them in Variable</span>                <span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>                    inputs <span class="token operator">=</span> Variable<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    labels <span class="token operator">=</span> Variable<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> Variable<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> Variable<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># zero the parameter gradients</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># forward</span>                outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>                _<span class="token punctuation">,</span> preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>                loss <span class="token operator">=</span> lossfunc<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># backward + optimize only if in training phase</span>                <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># statistics</span>                running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>data                running_corrects <span class="token operator">+=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>preds <span class="token operator">==</span> labels<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>            epoch_loss <span class="token operator">=</span> running_loss <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>            epoch_acc <span class="token operator">=</span> running_corrects <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{} Loss: {:.4f} Acc: {:.4f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>                phase<span class="token punctuation">,</span> epoch_loss<span class="token punctuation">,</span> epoch_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># deep copy the model</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'val'</span> <span class="token operator">and</span> epoch_acc <span class="token operator">></span> best_acc<span class="token punctuation">:</span>                best_acc <span class="token operator">=</span> epoch_acc                best_model_wts <span class="token operator">=</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>    elapsed_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Training complete in {:.0f}m {:.0f}s'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>        elapsed_time <span class="token operator">//</span> <span class="token number">60</span><span class="token punctuation">,</span> elapsed_time <span class="token operator">%</span> <span class="token number">60</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Best val Acc: {:4f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>best_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># load best model weights</span>    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>best_model_wts<span class="token punctuation">)</span>    <span class="token keyword">return</span> model<span class="token comment" spellcheck="true"># get model and replace the original fc layer with your fc layer</span>model_ft <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>num_ftrs <span class="token operator">=</span> model_ft<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>in_featuresmodel_ft<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_ftrs<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>    model_ft <span class="token operator">=</span> model_ft<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># define loss function</span>lossfunc <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># setting optimizer and trainable parameters</span> <span class="token comment" spellcheck="true">#   params = model_ft.parameters()</span> <span class="token comment" spellcheck="true"># list(model_ft.fc.parameters())+list(model_ft.layer4.parameters())</span><span class="token comment" spellcheck="true">#params = list(model_ft.fc.parameters())+list( model_ft.parameters())</span>params <span class="token operator">=</span> list<span class="token punctuation">(</span>model_ft<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>optimizer_ft <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Decay LR by a factor of 0.1 every 7 epochs</span>exp_lr_scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer_ft<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>model_ft <span class="token operator">=</span> train_model<span class="token punctuation">(</span>model<span class="token operator">=</span>model_ft<span class="token punctuation">,</span>                           lossfunc<span class="token operator">=</span>lossfunc<span class="token punctuation">,</span>                           optimizer<span class="token operator">=</span>optimizer_ft<span class="token punctuation">,</span>                           scheduler<span class="token operator">=</span>exp_lr_scheduler<span class="token punctuation">,</span>                           num_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span></code></pre><pre><code>Epoch 0/4----------C:\Users\16413\anaconda3\lib\site-packages\torch\optim\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate  &quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;, UserWarning)train Loss: 0.0750 Acc: 0.6700val Loss: 0.0436 Acc: 0.8200Epoch 1/4----------train Loss: 0.0399 Acc: 0.8250val Loss: 0.0345 Acc: 0.8470Epoch 2/4----------train Loss: 0.0330 Acc: 0.8473val Loss: 0.0303 Acc: 0.8610Epoch 3/4----------train Loss: 0.0300 Acc: 0.8575val Loss: 0.0293 Acc: 0.8650Epoch 4/4----------train Loss: 0.0288 Acc: 0.8643val Loss: 0.0281 Acc: 0.8750Training complete in 6m 31sBest val Acc: 0.875000</code></pre><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model_ft<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'./model.pth'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"done"</span><span class="token punctuation">)</span></code></pre><pre><code>done</code></pre><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>热爱是一件很好的事。</p><p>读者可以百度或知乎一下，深度学习、神经网络、resnet等一堆概念。<br>明天，进攻pytorch官方文档。<br>本人第一篇Blog,不要介意哈。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> torch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/3eeb.html"/>
      <url>/posts/3eeb.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
