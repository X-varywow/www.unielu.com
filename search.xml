<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用ImageNet预训练模型权重</title>
      <link href="/posts/af40.html"/>
      <url>/posts/af40.html</url>
      
        <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>使用ImageNet预训练模型权重<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#使用ImageNet预训练模型权重" target="_blank" rel="noopener"></a></p><p>Keras框架提供了预置好的VGG16模型，并提供使用ImageNet进行大规模训练的权重。我们使用预置的VGG16模型，加载预训练的权重，保留VGG16的卷积网络结构，只在最后的全连接层进行调整，使用GlobalAveragePooling2D将7x7x512的卷积结果进行全局池化，减少训练参数，并加入softmax激活、output shape为2的全连接层进行二分类。</p><p>首先，我们准备数据：</p><p>In [1]:</p><pre><code>!pip install --upgrade keras_applications==1.0.6 keras==2.2.4import osif os.path.exists(&#39;./data&#39;) == False:    from modelarts.session import Session    session = Session()    if session.region_name == &#39;cn-north-1&#39;:        bucket_path=&quot;modelarts-labs/notebook/DL_image_recognition/image_recognition.tar.gz&quot;    elif session.region_name == &#39;cn-north-4&#39;:        bucket_path=&quot;modelarts-labs-bj4/notebook/DL_image_recognition/image_recognition.tar.gz&quot;    else:        print(&quot;请更换地区到北京一或北京四&quot;)    session.download_data(    bucket_path=bucket_path,    path=&quot;./image_recognition.tar.gz&quot;)    # 使用tar命令解压资源包    !tar xf ./image_recognition.tar.gz    # 清理压缩包    !rm -f ./image_recognition.tar.gz```</code></pre><p>Looking in indexes: <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a><br>Requirement already up-to-date: keras_applications==1.0.6 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (1.0.6)<br>Requirement already up-to-date: keras==2.2.4 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (2.2.4)<br>Requirement already satisfied, skipping upgrade: h5py in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras_applications==1.0.6) (2.9.0)<br>Requirement already satisfied, skipping upgrade: numpy&gt;=1.9.1 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras_applications==1.0.6) (1.15.4)<br>Requirement already satisfied, skipping upgrade: six&gt;=1.9.0 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.12.0)<br>Requirement already satisfied, skipping upgrade: keras-preprocessing&gt;=1.0.5 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.0.5)<br>Requirement already satisfied, skipping upgrade: scipy&gt;=0.14 in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (1.2.0)<br>Requirement already satisfied, skipping upgrade: pyyaml in /opt/anaconda/envs/mask-rcnn/lib/python3.6/site-packages (from keras==2.2.4) (3.13)</p><pre><code>### 导入相关库[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#导入相关库)In \[2\]:</code></pre><p>from keras.applications.vgg16 import VGG16from keras.preprocessing import imageimport numpy as np</p><p>from keras.preprocessing import imagefrom keras.models import Modelfrom keras.layers import Dense, GlobalAveragePooling2Dfrom keras import backend as Kfrom keras.models import load_model</p><p>from keras.preprocessing.image import ImageDataGenerator```</p><pre><code>Using TensorFlow backend.</code></pre><h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#准备数据集" target="_blank" rel="noopener"></a></h3><p>In [3]:</p><pre><code>import osfrom PIL import Imagedef load_data():    dirname = &quot;./data&quot;    path = &quot;./data&quot;    num_train_samples = 25000    x_train = np.empty((num_train_samples, 224,224,3), dtype=&#39;uint8&#39;)    y_train = np.empty((num_train_samples,1), dtype=&#39;uint8&#39;)    index = 0    for file in os.listdir(&quot;./data&quot;):        image = Image.open(os.path.join(dirname,file)).resize((224,224))        image = np.array(image)        x_train[index,:,:,:] = image        if &quot;cat&quot; in file:            y_train[index,0] =1        elif &quot;dog&quot; in file:            y_train[index,0] =0        index += 1    return (x_train, y_train)```In \[4\]:</code></pre><p>(x_train, y_train) = load_data()```<br>In [5]:</p><pre><code>print(x_train.shape)print(y_train.shape)```</code></pre><p>(25000, 224, 224, 3)<br>(25000, 1)</p><pre><code>In \[6\]:</code></pre><p>from keras.utils import np_utilsdef process_data(x_train,y_train):<br>    x_train = x_train.astype(np.float32)<br>    x_train /= 255<br>    n_classes = 2<br>    y_train = np_utils.to_categorical(y_train, n_classes)<br>    return x_train,y_train```<br>In [24]:</p><pre><code>def build_model(base_model):    x = base_model.output    x = GlobalAveragePooling2D()(x)    predictions = Dense(2, activation=&#39;softmax&#39;)(x)    model = Model(inputs=base_model.input, outputs=predictions)    print(type(model))    return model```In \[8\]:</code></pre><p>x_train,y_train= process_data(x_train,y_train)print(x_train.shape)print(y_train.shape)```</p><pre><code>(25000, 224, 224, 3)(25000, 2)</code></pre><h3 id="准备模型"><a href="#准备模型" class="headerlink" title="准备模型"></a>准备模型<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#准备模型" target="_blank" rel="noopener"></a></h3><p>我们定义输入数据的维度，并构建VGG16模型，加载ImageNet预训练的权重（如果预训练权重不存在，则从网络下载，并保存到到$HOME目录的.keras/models/路径下）。include_top=False表示只取卷积网络结构中的参数，不包含全连接层和softmax分类层（ImageNet有1000个分类）。</p><p>In [9]:</p><pre><code>base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False)```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Colocations handled automatically by placer.</p><pre><code>我们把所有卷基层的trainable设置为False，不进行训练。然后将池化层和全连接的二分类层添加到模型中，输入层不变In \[10\]:</code></pre><p>for layer in base_model.layers:<br>    layer.trainable = False</p><p>model = build_model(base_model)model.summary()```</p><pre><code>&lt;class &#39;keras.engine.training.Model&#39;&gt;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, None, None, 3)     0         _________________________________________________________________block1_conv1 (Conv2D)        (None, None, None, 64)    1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, None, None, 64)    36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, None, None, 64)    0         _________________________________________________________________block2_conv1 (Conv2D)        (None, None, None, 128)   73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, None, None, 128)   147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, None, None, 128)   0         _________________________________________________________________block3_conv1 (Conv2D)        (None, None, None, 256)   295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, None, None, 256)   0         _________________________________________________________________block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________global_average_pooling2d_1 ( (None, 512)               0         _________________________________________________________________dense_1 (Dense)              (None, 2)                 1026      =================================================================Total params: 14,715,714Trainable params: 1,026Non-trainable params: 14,714,688_________________________________________________________________</code></pre><p>可以看到，训练的参数个数为1026，我们仅训练分类部分。</p><h3 id="设置模型的损失函数和优化器"><a href="#设置模型的损失函数和优化器" class="headerlink" title="设置模型的损失函数和优化器"></a>设置模型的损失函数和优化器<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#设置模型的损失函数和优化器" target="_blank" rel="noopener"></a></h3><p>In [11]:</p><pre><code>import keras opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)model.compile(loss=&#39;binary_crossentropy&#39;,              optimizer=opt,              metrics=[&#39;accuracy&#39;])```### 设置callback并训练模型[](https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&amp;nwo=huaweicloud/ModelArts-Lab&amp;path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&amp;repository_id=185701977&amp;repository_type=Repository#设置callback并训练模型)使用预训练的权重，可以较快提高模型精度。在高性能GPU环境下，每一轮需要几分钟左右，请实践者保持耐心，笔者的实践中训练到第11轮就达到了约90%的精度，耗时约8分钟。后续训练精度提升开始变慢，在41轮达到约92%的精度，耗时约30分钟。In \[12\]:</code></pre><p>from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateaues = EarlyStopping(monitor=’val_acc’, baseline=0.9, patience=15, verbose=1, mode=’auto’)cp = ModelCheckpoint(filepath=”./ckpt_vgg16_dog_and_cat.h5”, monitor=”val_acc”, verbose=1, save_best_only=True, mode=”auto”, period=1)lr = ReduceLROnPlateau(monitor=”val_acc”, factor=0.1, patience=10, verbose=1, mode=”auto”, min_lr=0)callbacks = [es,cp,lr]```<br>In [13]:</p><pre><code>history = model.fit(x=x_train,                     y=y_train,                     batch_size=16,                     epochs=100,                     verbose=1,                     callbacks=callbacks,                     validation_split=0.25,                     shuffle=True,                     initial_epoch=0)```</code></pre><p>WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Use tf.cast instead.<br>Train on 18750 samples, validate on 6250 samples<br>Epoch 1/100<br>18750/18750 [==============================] - 48s 3ms/step - loss: 0.6284 - acc: 0.6794 - val_loss: 0.5503 - val_acc: 0.7995</p><p>Epoch 00001: val_acc improved from -inf to 0.79952, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 2/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.4959 - acc: 0.8314 - val_loss: 0.4565 - val_acc: 0.8410</p><p>Epoch 00002: val_acc improved from 0.79952 to 0.84096, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 3/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.4233 - acc: 0.8564 - val_loss: 0.4012 - val_acc: 0.8630</p><p>Epoch 00003: val_acc improved from 0.84096 to 0.86304, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 4/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3789 - acc: 0.8707 - val_loss: 0.3669 - val_acc: 0.8710</p><p>Epoch 00004: val_acc improved from 0.86304 to 0.87104, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 5/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3493 - acc: 0.8793 - val_loss: 0.3417 - val_acc: 0.8824</p><p>Epoch 00005: val_acc improved from 0.87104 to 0.88240, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 6/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3274 - acc: 0.8848 - val_loss: 0.3233 - val_acc: 0.8867</p><p>Epoch 00006: val_acc improved from 0.88240 to 0.88672, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 7/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.3105 - acc: 0.8903 - val_loss: 0.3089 - val_acc: 0.8864</p><p>Epoch 00007: val_acc did not improve from 0.88672<br>Epoch 8/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2975 - acc: 0.8933 - val_loss: 0.2966 - val_acc: 0.8909</p><p>Epoch 00008: val_acc improved from 0.88672 to 0.89088, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 9/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2867 - acc: 0.8960 - val_loss: 0.2869 - val_acc: 0.8979</p><p>Epoch 00009: val_acc improved from 0.89088 to 0.89792, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 10/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2778 - acc: 0.8975 - val_loss: 0.2797 - val_acc: 0.8990</p><p>Epoch 00010: val_acc improved from 0.89792 to 0.89904, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 11/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2702 - acc: 0.9004 - val_loss: 0.2721 - val_acc: 0.9016</p><p>Epoch 00011: val_acc improved from 0.89904 to 0.90160, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 12/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2637 - acc: 0.9020 - val_loss: 0.2658 - val_acc: 0.9011</p><p>Epoch 00012: val_acc did not improve from 0.90160<br>Epoch 13/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2581 - acc: 0.9029 - val_loss: 0.2605 - val_acc: 0.9034</p><p>Epoch 00013: val_acc improved from 0.90160 to 0.90336, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 14/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2530 - acc: 0.9045 - val_loss: 0.2559 - val_acc: 0.9045</p><p>Epoch 00014: val_acc improved from 0.90336 to 0.90448, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 15/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2486 - acc: 0.9055 - val_loss: 0.2518 - val_acc: 0.9064</p><p>Epoch 00015: val_acc improved from 0.90448 to 0.90640, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 16/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2445 - acc: 0.9074 - val_loss: 0.2480 - val_acc: 0.9070</p><p>Epoch 00016: val_acc improved from 0.90640 to 0.90704, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 17/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2410 - acc: 0.9084 - val_loss: 0.2447 - val_acc: 0.9083</p><p>Epoch 00017: val_acc improved from 0.90704 to 0.90832, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 18/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2377 - acc: 0.9090 - val_loss: 0.2417 - val_acc: 0.9083</p><p>Epoch 00018: val_acc did not improve from 0.90832<br>Epoch 19/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2346 - acc: 0.9106 - val_loss: 0.2398 - val_acc: 0.9078</p><p>Epoch 00019: val_acc did not improve from 0.90832<br>Epoch 20/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2319 - acc: 0.9115 - val_loss: 0.2362 - val_acc: 0.9107</p><p>Epoch 00020: val_acc improved from 0.90832 to 0.91072, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 21/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2293 - acc: 0.9118 - val_loss: 0.2339 - val_acc: 0.9107</p><p>Epoch 00021: val_acc did not improve from 0.91072<br>Epoch 22/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2270 - acc: 0.9122 - val_loss: 0.2318 - val_acc: 0.9114</p><p>Epoch 00022: val_acc improved from 0.91072 to 0.91136, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 23/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2247 - acc: 0.9132 - val_loss: 0.2299 - val_acc: 0.9114</p><p>Epoch 00023: val_acc did not improve from 0.91136<br>Epoch 24/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2228 - acc: 0.9135 - val_loss: 0.2278 - val_acc: 0.9125</p><p>Epoch 00024: val_acc improved from 0.91136 to 0.91248, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 25/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2209 - acc: 0.9147 - val_loss: 0.2265 - val_acc: 0.9122</p><p>Epoch 00025: val_acc did not improve from 0.91248<br>Epoch 26/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2190 - acc: 0.9151 - val_loss: 0.2243 - val_acc: 0.9120</p><p>Epoch 00026: val_acc did not improve from 0.91248<br>Epoch 27/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2173 - acc: 0.9155 - val_loss: 0.2227 - val_acc: 0.9141</p><p>Epoch 00027: val_acc improved from 0.91248 to 0.91408, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 28/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2158 - acc: 0.9163 - val_loss: 0.2213 - val_acc: 0.9142</p><p>Epoch 00028: val_acc improved from 0.91408 to 0.91424, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 29/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2142 - acc: 0.9166 - val_loss: 0.2199 - val_acc: 0.9139</p><p>Epoch 00029: val_acc did not improve from 0.91424<br>Epoch 30/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2129 - acc: 0.9174 - val_loss: 0.2187 - val_acc: 0.9144</p><p>Epoch 00030: val_acc improved from 0.91424 to 0.91440, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 31/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2114 - acc: 0.9179 - val_loss: 0.2173 - val_acc: 0.9149</p><p>Epoch 00031: val_acc improved from 0.91440 to 0.91488, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 32/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2101 - acc: 0.9169 - val_loss: 0.2170 - val_acc: 0.9154</p><p>Epoch 00032: val_acc improved from 0.91488 to 0.91536, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 33/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2090 - acc: 0.9179 - val_loss: 0.2151 - val_acc: 0.9170</p><p>Epoch 00033: val_acc improved from 0.91536 to 0.91696, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 34/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2079 - acc: 0.9193 - val_loss: 0.2145 - val_acc: 0.9173</p><p>Epoch 00034: val_acc improved from 0.91696 to 0.91728, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 35/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2068 - acc: 0.9182 - val_loss: 0.2130 - val_acc: 0.9171</p><p>Epoch 00035: val_acc did not improve from 0.91728<br>Epoch 36/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2057 - acc: 0.9198 - val_loss: 0.2122 - val_acc: 0.9176</p><p>Epoch 00036: val_acc improved from 0.91728 to 0.91760, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 37/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2047 - acc: 0.9196 - val_loss: 0.2114 - val_acc: 0.9187</p><p>Epoch 00037: val_acc improved from 0.91760 to 0.91872, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 38/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2038 - acc: 0.9199 - val_loss: 0.2103 - val_acc: 0.9182</p><p>Epoch 00038: val_acc did not improve from 0.91872<br>Epoch 39/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2027 - acc: 0.9206 - val_loss: 0.2096 - val_acc: 0.9189</p><p>Epoch 00039: val_acc improved from 0.91872 to 0.91888, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 40/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2019 - acc: 0.9204 - val_loss: 0.2090 - val_acc: 0.9178</p><p>Epoch 00040: val_acc did not improve from 0.91888<br>Epoch 41/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2011 - acc: 0.9205 - val_loss: 0.2082 - val_acc: 0.9202</p><p>Epoch 00041: val_acc improved from 0.91888 to 0.92016, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 42/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.2003 - acc: 0.9218 - val_loss: 0.2072 - val_acc: 0.9195</p><p>Epoch 00042: val_acc did not improve from 0.92016<br>Epoch 43/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1994 - acc: 0.9220 - val_loss: 0.2068 - val_acc: 0.9202</p><p>Epoch 00043: val_acc did not improve from 0.92016<br>Epoch 44/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1987 - acc: 0.9216 - val_loss: 0.2059 - val_acc: 0.9184</p><p>Epoch 00044: val_acc did not improve from 0.92016<br>Epoch 45/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1979 - acc: 0.9214 - val_loss: 0.2065 - val_acc: 0.9189</p><p>Epoch 00045: val_acc did not improve from 0.92016<br>Epoch 46/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1960 - acc: 0.9229 - val_loss: 0.2033 - val_acc: 0.9194</p><p>Epoch 00048: val_acc did not improve from 0.92048<br>Epoch 49/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1953 - acc: 0.9226 - val_loss: 0.2037 - val_acc: 0.9192</p><p>Epoch 00049: val_acc did not improve from 0.92048<br>Epoch 50/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1946 - acc: 0.9226 - val_loss: 0.2029 - val_acc: 0.9189</p><p>Epoch 00050: val_acc did not improve from 0.92048<br>Epoch 51/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1942 - acc: 0.9232 - val_loss: 0.2021 - val_acc: 0.9211</p><p>Epoch 00051: val_acc improved from 0.92048 to 0.92112, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 52/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1934 - acc: 0.9231 - val_loss: 0.2015 - val_acc: 0.9211</p><p>Epoch 00052: val_acc did not improve from 0.92112<br>Epoch 53/100<br>13264/18750 [====================&gt;………] - ETA: 9s - loss: 0.1921 - acc: 0.9226```</p><pre><code>IOPub message rate exceeded.The notebook server will temporarily stop sending outputto the client in order to avoid crashing it.To change this limit, set the config variable`--NotebookApp.iopub_msg_rate_limit`.Current values:NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)NotebookApp.rate_limit_window=3.0 (secs)</code></pre><pre><code>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1882 - acc: 0.9254 - val_loss: 0.1966 - val_acc: 0.9210Epoch 00063: val_acc did not improve from 0.92208Epoch 64/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1876 - acc: 0.9256 - val_loss: 0.1963 - val_acc: 0.9210Epoch 00064: val_acc did not improve from 0.92208Epoch 65/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1873 - acc: 0.9251 - val_loss: 0.1968 - val_acc: 0.9230Epoch 00065: val_acc improved from 0.92208 to 0.92304, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 66/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1870 - acc: 0.9254 - val_loss: 0.1958 - val_acc: 0.9205Epoch 00066: val_acc did not improve from 0.92304Epoch 67/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1866 - acc: 0.9256 - val_loss: 0.1953 - val_acc: 0.9214Epoch 00067: val_acc did not improve from 0.92304Epoch 68/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1862 - acc: 0.9265 - val_loss: 0.1956 - val_acc: 0.9230Epoch 00068: val_acc did not improve from 0.92304Epoch 69/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1858 - acc: 0.9265 - val_loss: 0.1947 - val_acc: 0.9213Epoch 00069: val_acc did not improve from 0.92304Epoch 70/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1854 - acc: 0.9255 - val_loss: 0.1950 - val_acc: 0.9229Epoch 00070: val_acc did not improve from 0.92304Epoch 71/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1852 - acc: 0.9268 - val_loss: 0.1941 - val_acc: 0.9216Epoch 00071: val_acc did not improve from 0.92304Epoch 72/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1848 - acc: 0.9268 - val_loss: 0.1940 - val_acc: 0.9232Epoch 00072: val_acc improved from 0.92304 to 0.92320, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 73/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1845 - acc: 0.9274 - val_loss: 0.1937 - val_acc: 0.9230Epoch 00073: val_acc did not improve from 0.92320Epoch 74/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1841 - acc: 0.9266 - val_loss: 0.1933 - val_acc: 0.9206Epoch 00074: val_acc did not improve from 0.92320Epoch 75/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1838 - acc: 0.9268 - val_loss: 0.1930 - val_acc: 0.9218Epoch 00075: val_acc did not improve from 0.92320Epoch 76/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1834 - acc: 0.9266 - val_loss: 0.1928 - val_acc: 0.9227Epoch 00076: val_acc did not improve from 0.92320Epoch 77/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1832 - acc: 0.9273 - val_loss: 0.1926 - val_acc: 0.9224Epoch 00077: val_acc did not improve from 0.92320Epoch 78/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1826 - acc: 0.9273 - val_loss: 0.1924 - val_acc: 0.9237Epoch 00078: val_acc improved from 0.92320 to 0.92368, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 79/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1824 - acc: 0.9277 - val_loss: 0.1921 - val_acc: 0.9229Epoch 00079: val_acc did not improve from 0.92368Epoch 80/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1821 - acc: 0.9285 - val_loss: 0.1920 - val_acc: 0.9234Epoch 00080: val_acc did not improve from 0.92368Epoch 81/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1820 - acc: 0.9279 - val_loss: 0.1916 - val_acc: 0.9232Epoch 00081: val_acc did not improve from 0.92368Epoch 82/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1816 - acc: 0.9272 - val_loss: 0.1924 - val_acc: 0.9238Epoch 00082: val_acc improved from 0.92368 to 0.92384, saving model to ./ckpt_vgg16_dog_and_cat.h5Epoch 83/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1814 - acc: 0.9282 - val_loss: 0.1911 - val_acc: 0.9226Epoch 00083: val_acc did not improve from 0.92384Epoch 84/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1811 - acc: 0.9273 - val_loss: 0.1909 - val_acc: 0.9222Epoch 00084: val_acc did not improve from 0.92384Epoch 85/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1808 - acc: 0.9278 - val_loss: 0.1908 - val_acc: 0.9219Epoch 00085: val_acc did not improve from 0.92384Epoch 86/10018750/18750 [==============================] - 45s 2ms/step - loss: 0.1804 - acc: 0.9276 - val_loss: 0.1903 - val_acc: 0.9227Epoch 00087: val_acc did not improve from 0.92384Epoch 88/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1801 - acc: 0.9286 - val_loss: 0.1901 - val_acc: 0.9227Epoch 00088: val_acc did not improve from 0.92384Epoch 89/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1797 - acc: 0.9284 - val_loss: 0.1913 - val_acc: 0.9234Epoch 00089: val_acc did not improve from 0.92384Epoch 90/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1795 - acc: 0.9279 - val_loss: 0.1897 - val_acc: 0.9230Epoch 00090: val_acc did not improve from 0.92384Epoch 91/10018750/18750 [==============================] - 44s 2ms/step - loss: 0.1795 - acc: 0.9284 - val_loss: 0.1896 - val_acc: 0.9224Epoch 00091: val_acc did not improve from 0.92384Epoch 92/10010608/18750 [===============&gt;..............] - ETA: 14s - loss: 0.1796 - acc: 0.9285```</code></pre><p>IOPub message rate exceeded.<br>The notebook server will temporarily stop sending output<br>to the client in order to avoid crashing it.<br>To change this limit, set the config variable<br><code>--NotebookApp.iopub_msg_rate_limit</code>.</p><p>Current values:<br>NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)<br>NotebookApp.rate_limit_window=3.0 (secs)</p><pre><code></code></pre><p>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1779 - acc: 0.9288 - val_loss: 0.1884 - val_acc: 0.9232</p><p>Epoch 00098: val_acc did not improve from 0.92464<br>Epoch 99/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1776 - acc: 0.9291 - val_loss: 0.1882 - val_acc: 0.9237</p><p>Epoch 00099: val_acc did not improve from 0.92464<br>Epoch 100/100<br>18750/18750 [==============================] - 44s 2ms/step - loss: 0.1774 - acc: 0.9292 - val_loss: 0.1880 - val_acc: 0.9243</p><p>Epoch 00100: val_acc did not improve from 0.92464</p><pre><code>In \[21\]:</code></pre><p>import matplotlib.pyplot as pltprint(history.history[‘val_acc’][0])# 绘制训练 &amp; 验证的准确率值plt.plot(history.history[‘acc’])plt.plot(history.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show()```</p><pre><code>0.7995200000190735</code></pre><p><img src="" alt=""><br>In [22]:</p><pre><code># 绘制训练 &amp; 验证的损失值plt.plot(history.history[&#39;loss&#39;])plt.plot(history.history[&#39;val_loss&#39;])plt.title(&#39;Model loss&#39;)plt.ylabel(&#39;Loss&#39;)plt.xlabel(&#39;Epoch&#39;)plt.legend([&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper left&#39;)plt.show()```![]()在上面的训练中，我们使用预训练模型的权重，并保留了卷积网络结构中的参数权重，只训练分类层的参数。可以看到，由于预训练模型已经具备了比较好的特征提取能力，初始时就能获得比较高的准确率。接下来，我们将卷积网络中的参数也一起加入训练。在预训练模型的特征提取能力的基础上开始训练，找到更适合猫狗二分类任务的参数权重：In \[31\]:</code></pre><p>base_model = VGG16(weights=’imagenet’, include_top=False)model = build_model(base_model)model.compile(loss=’binary_crossentropy’,<br>              optimizer=opt,<br>              metrics=[‘accuracy’])model.summary()```</p><pre><code>&lt;class &#39;keras.engine.training.Model&#39;&gt;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_5 (InputLayer)         (None, None, None, 3)     0         _________________________________________________________________block1_conv1 (Conv2D)        (None, None, None, 64)    1792      _________________________________________________________________block1_conv2 (Conv2D)        (None, None, None, 64)    36928     _________________________________________________________________block1_pool (MaxPooling2D)   (None, None, None, 64)    0         _________________________________________________________________block2_conv1 (Conv2D)        (None, None, None, 128)   73856     _________________________________________________________________block2_conv2 (Conv2D)        (None, None, None, 128)   147584    _________________________________________________________________block2_pool (MaxPooling2D)   (None, None, None, 128)   0         _________________________________________________________________block3_conv1 (Conv2D)        (None, None, None, 256)   295168    _________________________________________________________________block3_conv2 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_conv3 (Conv2D)        (None, None, None, 256)   590080    _________________________________________________________________block3_pool (MaxPooling2D)   (None, None, None, 256)   0         _________________________________________________________________block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   _________________________________________________________________block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block4_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   _________________________________________________________________block5_pool (MaxPooling2D)   (None, None, None, 512)   0         _________________________________________________________________global_average_pooling2d_5 ( (None, 512)               0         _________________________________________________________________dense_5 (Dense)              (None, 2)                 1026      =================================================================Total params: 14,715,714Trainable params: 14,715,714Non-trainable params: 0_________________________________________________________________</code></pre><p>可以看到，这次模型的Non-trainable params个数为0，所有参数都参与训练，相当于我们使用预训练的参数权重初始化了卷积部分。</p><h3 id="训练所有参数"><a href="#训练所有参数" class="headerlink" title="训练所有参数"></a>训练所有参数<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#训练所有参数" target="_blank" rel="noopener"></a></h3><p>直接在预训练权重的基础上进行训练，可以更快的让泛化性较好的权重收敛到适合猫狗二分类的权重，我们仅训练3轮就达到了约95%的精度，耗时约5分钟。</p><p>In [32]:</p><pre><code>history = model.fit(x=x_train,                     y=y_train,                     batch_size=16,                     epochs=5,                     verbose=1,                     callbacks=callbacks,                     validation_split=0.25,                     shuffle=True,                     initial_epoch=0)```</code></pre><p>Train on 18750 samples, validate on 6250 samples<br>Epoch 1/5<br>18750/18750 [==============================] - 103s 5ms/step - loss: 0.4086 - acc: 0.8029 - val_loss: 0.2317 - val_acc: 0.8989</p><p>Epoch 00001: val_acc did not improve from 0.95120<br>Epoch 2/5<br>18750/18750 [==============================] - 102s 5ms/step - loss: 0.2157 - acc: 0.9247 - val_loss: 0.2462 - val_acc: 0.9019</p><p>Epoch 00002: val_acc did not improve from 0.95120<br>Epoch 3/5<br>18750/18750 [==============================] - 101s 5ms/step - loss: 0.1959 - acc: 0.9379 - val_loss: 0.1228 - val_acc: 0.9555</p><p>Epoch 00003: val_acc improved from 0.95120 to 0.95552, saving model to ./ckpt_vgg16_dog_and_cat.h5<br>Epoch 4/5<br>18750/18750 [==============================] - 101s 5ms/step - loss: 0.1849 - acc: 0.9386 - val_loss: 0.3406 - val_acc: 0.9373</p><p>Epoch 00004: val_acc did not improve from 0.95552<br>Epoch 5/5<br>18750/18750 [==============================] - 101s 5ms/step - loss: 0.2295 - acc: 0.9324 - val_loss: 0.1599 - val_acc: 0.9496</p><p>Epoch 00005: val_acc did not improve from 0.95552</p><pre><code>In \[33\]:</code></pre><p>import matplotlib.pyplot as pltprint(history.history[‘val_acc’][0])# 绘制训练 &amp; 验证的准确率值plt.plot(history.history[‘acc’])plt.plot(history.history[‘val_acc’])plt.title(‘Model accuracy’)plt.ylabel(‘Accuracy’)plt.xlabel(‘Epoch’)plt.legend([‘Train’, ‘Test’], loc=’upper left’)plt.show();```</p><pre><code>0.8988799999618531</code></pre><p><img src="" alt=""><br>可以看到，模型在开始训练后就得到了较高的精度。在VGG论文中，作者也提到了使用预训练好的浅层VGG模型去初始化深层VGG模型的一部分层，再继续训练的技巧。在上面的实践中，我们使用了泛化性更强的参数权重初始化模型，并针对我们特定的任务进行训练，可以更快的得到较好的精度。</p><h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展<a href="https://render.githubusercontent.com/view/ipynb?commit=f4f272b8a9414e15ec8b460865e99d126c081f0f&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f687561776569636c6f75642f4d6f64656c417274732d4c61622f663466323732623861393431346531356563386234363038363565393964313236633038316630662f6e6f7465626f6f6b2f444c5f696d6167655f6879706572706172616d657465725f74756e696e672f30335f707265747261696e65645f776569676874732e6970796e62&nwo=huaweicloud/ModelArts-Lab&path=notebook/DL_image_hyperparameter_tuning/03_pretrained_weights.ipynb&repository_id=185701977&repository_type=Repository#扩展" target="_blank" rel="noopener"></a></h3><p>尝试使用更深层的网络，如VGG19, ResNet在ImageNet数据集预训练的权重，并针对猫狗二分类任务进行迁移学习，查看模型训练的效果。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> modelarts </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习第一天（pytorch学习笔记）</title>
      <link href="/posts/6080.html"/>
      <url>/posts/6080.html</url>
      
        <content type="html"><![CDATA[<h1 id="入坑（2020-4-29）"><a href="#入坑（2020-4-29）" class="headerlink" title="入坑（2020.4.29）"></a>入坑（2020.4.29）</h1><p>喜欢深度学习，喜欢python。<br>看起来torch挺好的。</p><h1 id="torch环境准备"><a href="#torch环境准备" class="headerlink" title="torch环境准备"></a>torch环境准备</h1><ul><li>torch  (1.5.0)</li><li>torchvision  (0.6.0)</li><li>cuda  (10.2)<br><img src="https://img-blog.csdnimg.cn/20200429183137908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTk3MzYzMA==,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>输入指令执行代码,速度不行的话复制下载链接迅雷打开，可能有惊喜<img src="https://img-blog.csdnimg.cn/20200429183453171.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTk3MzYzMA==,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>环境配好后，执行<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> pythontorch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>这时应该返回True.</li></ul><h1 id="一次比赛的代码"><a href="#一次比赛的代码" class="headerlink" title="一次比赛的代码"></a>一次比赛的代码</h1><p>不懂的话直接当普通代码执行，当然本人还是纯小白，代码是copy来的，一次比赛的baseline代码</p><h4 id="加载pytorch框架下的依赖项"><a href="#加载pytorch框架下的依赖项" class="headerlink" title="加载pytorch框架下的依赖项"></a>加载pytorch框架下的依赖项</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> print_function<span class="token punctuation">,</span> division<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> lr_scheduler<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token keyword">import</span> torchvision<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> models<span class="token punctuation">,</span> transforms<span class="token keyword">import</span> time<span class="token keyword">import</span> os</code></pre><h4 id="加载数据集，并分为训练集和测试集"><a href="#加载数据集，并分为训练集和测试集" class="headerlink" title="加载数据集，并分为训练集和测试集"></a>加载数据集，并分为训练集和测试集</h4><pre class=" language-python"><code class="language-python">dataTrans <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>            transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span>data_dir <span class="token operator">=</span> <span class="token string">'./images'</span>all_image_datasets <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> dataTrans<span class="token punctuation">)</span>trainsize <span class="token operator">=</span> int<span class="token punctuation">(</span><span class="token number">0.8</span><span class="token operator">*</span>len<span class="token punctuation">(</span>all_image_datasets<span class="token punctuation">)</span><span class="token punctuation">)</span>testsize <span class="token operator">=</span> len<span class="token punctuation">(</span>all_image_datasets<span class="token punctuation">)</span> <span class="token operator">-</span> trainsizetrain_dataset<span class="token punctuation">,</span> test_dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>all_image_datasets<span class="token punctuation">,</span><span class="token punctuation">[</span>trainsize<span class="token punctuation">,</span>testsize<span class="token punctuation">]</span><span class="token punctuation">)</span>image_datasets <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'train'</span><span class="token punctuation">:</span>train_dataset<span class="token punctuation">,</span><span class="token string">'val'</span><span class="token punctuation">:</span>test_dataset<span class="token punctuation">}</span>dataloders <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>image_datasets<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                 batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>                                                 shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                                                 num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>dataset_sizes <span class="token operator">=</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> len<span class="token punctuation">(</span>image_datasets<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true"># use gpu or not</span>use_gpu <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_model</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> lossfunc<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> scheduler<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    best_model_wts <span class="token operator">=</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>    best_acc <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch {}/{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> num_epochs <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-'</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Each epoch has a training and validation phase</span>        <span class="token keyword">for</span> phase <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Set model to training mode</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Set model to evaluate mode</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span>            running_corrects <span class="token operator">=</span> <span class="token number">0.0</span>            <span class="token comment" spellcheck="true"># Iterate over data.</span>            <span class="token keyword">for</span> data <span class="token keyword">in</span> dataloders<span class="token punctuation">[</span>phase<span class="token punctuation">]</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># get the inputs</span>                inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data                <span class="token comment" spellcheck="true"># wrap them in Variable</span>                <span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>                    inputs <span class="token operator">=</span> Variable<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    labels <span class="token operator">=</span> Variable<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> Variable<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> Variable<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># zero the parameter gradients</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># forward</span>                outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>                _<span class="token punctuation">,</span> preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>                loss <span class="token operator">=</span> lossfunc<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># backward + optimize only if in training phase</span>                <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>                    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># statistics</span>                running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>data                running_corrects <span class="token operator">+=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>preds <span class="token operator">==</span> labels<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>            epoch_loss <span class="token operator">=</span> running_loss <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>            epoch_acc <span class="token operator">=</span> running_corrects <span class="token operator">/</span> dataset_sizes<span class="token punctuation">[</span>phase<span class="token punctuation">]</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{} Loss: {:.4f} Acc: {:.4f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>                phase<span class="token punctuation">,</span> epoch_loss<span class="token punctuation">,</span> epoch_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># deep copy the model</span>            <span class="token keyword">if</span> phase <span class="token operator">==</span> <span class="token string">'val'</span> <span class="token operator">and</span> epoch_acc <span class="token operator">></span> best_acc<span class="token punctuation">:</span>                best_acc <span class="token operator">=</span> epoch_acc                best_model_wts <span class="token operator">=</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>    elapsed_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Training complete in {:.0f}m {:.0f}s'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>        elapsed_time <span class="token operator">//</span> <span class="token number">60</span><span class="token punctuation">,</span> elapsed_time <span class="token operator">%</span> <span class="token number">60</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Best val Acc: {:4f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>best_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># load best model weights</span>    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>best_model_wts<span class="token punctuation">)</span>    <span class="token keyword">return</span> model<span class="token comment" spellcheck="true"># get model and replace the original fc layer with your fc layer</span>model_ft <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>num_ftrs <span class="token operator">=</span> model_ft<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>in_featuresmodel_ft<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_ftrs<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>    model_ft <span class="token operator">=</span> model_ft<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># define loss function</span>lossfunc <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># setting optimizer and trainable parameters</span> <span class="token comment" spellcheck="true">#   params = model_ft.parameters()</span> <span class="token comment" spellcheck="true"># list(model_ft.fc.parameters())+list(model_ft.layer4.parameters())</span><span class="token comment" spellcheck="true">#params = list(model_ft.fc.parameters())+list( model_ft.parameters())</span>params <span class="token operator">=</span> list<span class="token punctuation">(</span>model_ft<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>optimizer_ft <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Decay LR by a factor of 0.1 every 7 epochs</span>exp_lr_scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer_ft<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>model_ft <span class="token operator">=</span> train_model<span class="token punctuation">(</span>model<span class="token operator">=</span>model_ft<span class="token punctuation">,</span>                           lossfunc<span class="token operator">=</span>lossfunc<span class="token punctuation">,</span>                           optimizer<span class="token operator">=</span>optimizer_ft<span class="token punctuation">,</span>                           scheduler<span class="token operator">=</span>exp_lr_scheduler<span class="token punctuation">,</span>                           num_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span></code></pre><pre><code>Epoch 0/4----------C:\Users\16413\anaconda3\lib\site-packages\torch\optim\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate  &quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;, UserWarning)train Loss: 0.0750 Acc: 0.6700val Loss: 0.0436 Acc: 0.8200Epoch 1/4----------train Loss: 0.0399 Acc: 0.8250val Loss: 0.0345 Acc: 0.8470Epoch 2/4----------train Loss: 0.0330 Acc: 0.8473val Loss: 0.0303 Acc: 0.8610Epoch 3/4----------train Loss: 0.0300 Acc: 0.8575val Loss: 0.0293 Acc: 0.8650Epoch 4/4----------train Loss: 0.0288 Acc: 0.8643val Loss: 0.0281 Acc: 0.8750Training complete in 6m 31sBest val Acc: 0.875000</code></pre><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model_ft<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'./model.pth'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"done"</span><span class="token punctuation">)</span></code></pre><pre><code>done</code></pre><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>热爱是一件很好的事。</p><p>读者可以百度或知乎一下，深度学习、神经网络、resnet等一堆概念。<br>明天，进攻pytorch官方文档。<br>本人第一篇Blog,不要介意哈。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> torch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/3eeb.html"/>
      <url>/posts/3eeb.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
